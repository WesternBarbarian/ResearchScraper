{
  "metadata": {
    "exported_at": "2025-01-31T17:22:04.394026",
    "paper_count": 13,
    "export_format_version": "1.0"
  },
  "papers": [
    {
      "title": "Hashtag Re-Appropriation for Audience Control on Recommendation-Driven\n  Social Media Xiaohongshu (rednote)",
      "authors": [
        "Ruyuan Wan",
        "Lingbo Tong",
        "Tiffany Knearem",
        "Toby Jia-Jun Li",
        "Ting-Hao 'Kenneth' Huang",
        "Qunfang Wu"
      ],
      "published": "2025-01-30T08:55:32Z",
      "summary": "Algorithms have played a central role in personalized recommendations on\nsocial media. However, they also present significant obstacles for content\ncreators trying to predict and manage their audience reach. This issue is\nparticularly challenging for marginalized groups seeking to maintain safe\nspaces. Our study explores how women on Xiaohongshu (rednote), a\nrecommendation-driven social platform, proactively re-appropriate hashtags\n(e.g., #Baby Supplemental Food) by using them in posts unrelated to their\nliteral meaning. The hashtags were strategically chosen from topics that would\nbe uninteresting to the male audience they wanted to block. Through a\nmixed-methods approach, we analyzed the practice of hashtag re-appropriation\nbased on 5,800 collected posts and interviewed 24 active users from diverse\nbackgrounds to uncover users' motivations and reactions towards the\nre-appropriation. This practice highlights how users can reclaim agency over\ncontent distribution on recommendation-driven platforms, offering insights into\nself-governance within algorithmic-centered power structures.",
      "link": "http://arxiv.org/abs/2501.18210v1",
      "categories": [
        "cs.HC",
        "cs.CY",
        "cs.IR",
        "cs.SI"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "From tools to thieves: Measuring and understanding public perceptions of\n  AI through crowdsourced metaphors",
      "authors": [
        "Myra Cheng",
        "Angela Y. Lee",
        "Kristina Rapuano",
        "Kate Niederhoffer",
        "Alex Liebscher",
        "Jeffrey Hancock"
      ],
      "published": "2025-01-29T23:17:43Z",
      "summary": "How has the public responded to the increasing prevalence of artificial\nintelligence (AI)-based technologies? We investigate public perceptions of AI\nby collecting over 12,000 responses over 12 months from a nationally\nrepresentative U.S. sample. Participants provided open-ended metaphors\nreflecting their mental models of AI, a methodology that overcomes the\nlimitations of traditional self-reported measures. Using a mixed-methods\napproach combining quantitative clustering and qualitative coding, we identify\n20 dominant metaphors shaping public understanding of AI. To analyze these\nmetaphors systematically, we present a scalable framework integrating language\nmodeling (LM)-based techniques to measure key dimensions of public perception:\nanthropomorphism (attribution of human-like qualities), warmth, and competence.\nWe find that Americans generally view AI as warm and competent, and that over\nthe past year, perceptions of AI's human-likeness and warmth have significantly\nincreased ($+34\\%, r = 0.80, p < 0.01; +41\\%, r = 0.62, p < 0.05$).\nFurthermore, these implicit perceptions, along with the identified dominant\nmetaphors, strongly predict trust in and willingness to adopt AI ($r^2 = 0.21,\n0.18, p < 0.001$). We further explore how differences in metaphors and implicit\nperceptions--such as the higher propensity of women, older individuals, and\npeople of color to anthropomorphize AI--shed light on demographic disparities\nin trust and adoption. In addition to our dataset and framework for tracking\nevolving public attitudes, we provide actionable insights on using metaphors\nfor inclusive and responsible AI development.",
      "link": "http://arxiv.org/abs/2501.18045v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "\"I Would Never Trust Anything Western\": Kumu (Educator) Perspectives on\n  Use of LLMs for Culturally Revitalizing CS Education in Hawaiian Schools",
      "authors": [
        "Manas Mhasakar",
        "Rachel Baker-Ramos",
        "Ben Carter",
        "Evyn-Bree Helekahi-Kaiwi",
        "Josiah Hester"
      ],
      "published": "2025-01-29T19:05:33Z",
      "summary": "As large language models (LLMs) become increasingly integrated into\neducational technology, their potential to assist in developing curricula has\ngained interest among educators. Despite this growing attention, their\napplicability in culturally responsive Indigenous educational settings like\nHawai`i's public schools and Kaiapuni (immersion language) programs, remains\nunderstudied. Additionally, `Olelo Hawai`i, the Hawaiian language, as a\nlow-resource language, poses unique challenges and concerns about cultural\nsensitivity and the reliability of generated content. Through surveys and\ninterviews with kumu (educators), this study explores the perceived benefits\nand limitations of using LLMs for culturally revitalizing computer science (CS)\neducation in Hawaiian public schools with Kaiapuni programs. Our findings\nhighlight AI's time-saving advantages while exposing challenges such as\ncultural misalignment and reliability concerns. We conclude with design\nrecommendations for future AI tools to better align with Hawaiian cultural\nvalues and pedagogical practices, towards the broader goal of trustworthy,\neffective, and culturally grounded AI technologies.",
      "link": "http://arxiv.org/abs/2501.17942v1",
      "categories": [
        "cs.CY",
        "cs.HC"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "In-IDE Programming Courses: Learning Software Development in a\n  Real-World Setting",
      "authors": [
        "Anastasiia Birillo",
        "Ilya Vlasov",
        "Katsiaryna Dzialets",
        "Hieke Keuning",
        "Timofey Bryksin"
      ],
      "published": "2025-01-29T16:34:22Z",
      "summary": "While learning programming languages is crucial for software engineers,\nmastering the necessary tools is equally important. To facilitate this,\nJetBrains recently released the JetBrains Academy plugin, which customizes the\nIDE for learners, allowing tutors to create courses entirely within IDE.\n  In this work, we provide the first exploratory study of this learning format.\nWe carried out eight one-hour interviews with students and developers who\ncompleted at least one course using the plugin, inquiring about their\nexperience with the format, the used IDE features, and the current\nshortcomings. Our results indicate that learning inside the IDE is overall\nwelcomed by the learners, allowing them to study in a more realistic setting,\nusing features such as debugging and code analysis, which are crucial for real\nsoftware development. With the collected results and the analysis of the\ncurrent drawbacks, we aim to contribute to teaching students more practical\nskills.",
      "link": "http://arxiv.org/abs/2501.17747v1",
      "categories": [
        "cs.SE",
        "cs.CY",
        "cs.HC"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "The Imitation Game According To Turing",
      "authors": [
        "Sharon Temtsin",
        "Diane Proudfoot",
        "David Kaber",
        "Christoph Bartneck"
      ],
      "published": "2025-01-29T13:08:17Z",
      "summary": "The current cycle of hype and anxiety concerning the benefits and risks to\nhuman society of Artificial Intelligence is fuelled, not only by the increasing\nuse of generative AI and other AI tools by the general public, but also by\nclaims made on behalf of such technology by popularizers and scientists. In\nparticular, recent studies have claimed that Large Language Models (LLMs) can\npass the Turing Test-a goal for AI since the 1950s-and therefore can \"think\".\nLarge-scale impacts on society have been predicted as a result. Upon detailed\nexamination, however, none of these studies has faithfully applied Turing's\noriginal instructions. Consequently, we conducted a rigorous Turing Test with\nGPT-4-Turbo that adhered closely to Turing's instructions for a three-player\nimitation game. We followed established scientific standards where Turing's\ninstructions were ambiguous or missing. For example, we performed a\nComputer-Imitates-Human Game (CIHG) without constraining the time duration and\nconducted a Man-Imitates-Woman Game (MIWG) as a benchmark. All but one\nparticipant correctly identified the LLM, showing that one of today's most\nadvanced LLMs is unable to pass a rigorous Turing Test. We conclude that recent\nextravagant claims for such models are unsupported, and do not warrant either\noptimism or concern about the social impact of thinking machines.",
      "link": "http://arxiv.org/abs/2501.17629v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "Throwaway Accounts and Moderation on Reddit",
      "authors": [
        "Cheng Guo",
        "Kelly Caine"
      ],
      "published": "2025-01-29T06:10:10Z",
      "summary": "Social media platforms (SMPs) facilitate information sharing across varying\nlevels of sensitivity. A crucial design decision for SMP administrators is the\nplatform's identity policy, with some opting for real-name systems while others\nallow anonymous participation. Content moderation on these platforms is\nconducted by both humans and automated bots. This paper examines the\nrelationship between anonymity, specifically through the use of ``throwaway''\naccounts, and the extent and nature of content moderation on Reddit. Our\nfindings indicate that content originating from anonymous throwaway accounts is\nmore likely to violate rules on Reddit. Thus, they are more likely to be\nremoved by moderation than standard pseudonymous accounts. However, the\nmoderation actions applied to throwaway accounts are consistent with those\napplied to ordinary accounts, suggesting that the use of anonymous accounts\ndoes not necessarily necessitate increased human moderation. We conclude by\ndiscussing the implications of these findings for identity policies and content\nmoderation strategies on SMPs.",
      "link": "http://arxiv.org/abs/2501.17430v1",
      "categories": [
        "cs.HC",
        "cs.CY"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "The Right to AI",
      "authors": [
        "Rashid Mushkani",
        "Hugo Berard",
        "Allison Cohen",
        "Shin Koeski"
      ],
      "published": "2025-01-29T04:32:41Z",
      "summary": "This paper proposes a Right to AI, which asserts that individuals and\ncommunities should meaningfully participate in the development and governance\nof the AI systems that shape their lives. Motivated by the increasing\ndeployment of AI in critical domains and inspired by Henri Lefebvre's concept\nof the Right to the City, we reconceptualize AI as a societal infrastructure,\nrather than merely a product of expert design. In this paper, we critically\nevaluate how generative agents, large-scale data extraction, and diverse\ncultural values bring new complexities to AI oversight. The paper proposes that\ngrassroots participatory methodologies can mitigate biased outcomes and enhance\nsocial responsiveness. It asserts that data is socially produced and should be\nmanaged and owned collectively. Drawing on Sherry Arnstein's Ladder of Citizen\nParticipation and analyzing nine case studies, the paper develops a four-tier\nmodel for the Right to AI that situates the current paradigm and envisions an\naspirational future. It proposes recommendations for inclusive data ownership,\ntransparent design processes, and stakeholder-driven oversight. We also discuss\nmarket-led and state-centric alternatives and argue that participatory\napproaches offer a better balance between technical efficiency and democratic\nlegitimacy.",
      "link": "http://arxiv.org/abs/2501.17899v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "\"Ownership, Not Just Happy Talk\": Co-Designing a Participatory Large\n  Language Model for Journalism",
      "authors": [
        "Emily Tseng",
        "Meg Young",
        "Marianne Aubin Le Quéré",
        "Aimee Rinehart",
        "Harini Suresh"
      ],
      "published": "2025-01-28T21:06:52Z",
      "summary": "Journalism has emerged as an essential domain for understanding the uses,\nlimitations, and impacts of large language models (LLMs) in the workplace. News\norganizations face divergent financial incentives: LLMs already permeate\nnewswork processes within financially constrained organizations, even as\nongoing legal challenges assert that AI companies violate their copyright. At\nstake are key questions about what LLMs are created to do, and by whom: How\nmight a journalist-led LLM work, and what can participatory design illuminate\nabout the present-day challenges about adapting ``one-size-fits-all''\nfoundation models to a given context of use? In this paper, we undertake a\nco-design exploration to understand how a participatory approach to LLMs might\naddress opportunities and challenges around AI in journalism. Our 20 interviews\nwith reporters, data journalists, editors, labor organizers, product leads, and\nexecutives highlight macro, meso, and micro tensions that designing for this\nopportunity space must address. From these desiderata, we describe the result\nof our co-design work: organizational structures and functionality for a\njournalist-controlled LLM. In closing, we discuss the limitations of commercial\nfoundation models for workplace use, and the methodological implications of\napplying participatory methods to LLM co-design.",
      "link": "http://arxiv.org/abs/2501.17299v1",
      "categories": [
        "cs.HC",
        "cs.CL",
        "cs.CY"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "Standardised schema and taxonomy for AI incident databases in critical\n  digital infrastructure",
      "authors": [
        "Avinash Agarwal",
        "Manisha J. Nene"
      ],
      "published": "2025-01-28T15:59:01Z",
      "summary": "The rapid deployment of Artificial Intelligence (AI) in critical digital\ninfrastructure introduces significant risks, necessitating a robust framework\nfor systematically collecting AI incident data to prevent future incidents.\nExisting databases lack the granularity as well as the standardized structure\nrequired for consistent data collection and analysis, impeding effective\nincident management. This work proposes a standardized schema and taxonomy for\nAI incident databases, addressing these challenges by enabling detailed and\nstructured documentation of AI incidents across sectors. Key contributions\ninclude developing a unified schema, introducing new fields such as incident\nseverity, causes, and harms caused, and proposing a taxonomy for classifying AI\nincidents in critical digital infrastructure. The proposed solution facilitates\nmore effective incident data collection and analysis, thus supporting\nevidence-based policymaking, enhancing industry safety measures, and promoting\ntransparency. This work lays the foundation for a coordinated global response\nto AI incidents, ensuring trust, safety, and accountability in using AI across\nregions.",
      "link": "http://arxiv.org/abs/2501.17037v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "\"My Whereabouts, my Location, it's Directly Linked to my Physical\n  Security\": An Exploratory Qualitative Study of Location-Dependent Security\n  and Privacy Perceptions among Activist Tech Users",
      "authors": [
        "Christian Eichenmüller",
        "Lisa Kuhn",
        "Zinaida Benenson"
      ],
      "published": "2025-01-28T12:13:53Z",
      "summary": "Digital-safety research with at-risk users is particularly urgent. At-risk\nusers are more likely to be digitally attacked or targeted by surveillance and\ncould be disproportionately harmed by attacks that facilitate physical\nassaults. One group of such at-risk users are activists and politically active\nindividuals. For them, as for other at-risk users, the rise of smart\nenvironments harbors new risks. Since digitization and datafication are no\nlonger limited to a series of personal devices that can be switched on and off,\nbut increasingly and continuously surround users, granular geolocation poses\nnew safety challenges. Drawing on eight exploratory qualitative interviews of\nan ongoing research project, this contribution highlights what activists with\npowerful adversaries think about evermore data traces, including location data,\nand how they intend to deal with emerging risks. Responses of activists include\nattempts to control one's immediate technological surroundings and to more\ncarefully manage device-related location data. For some activists, threat\nmodeling has also shaped provider choices based on geopolitical considerations.\nSince many activists have not enough digital-safety knowledge for effective\nprotection, feelings of insecurity and paranoia are widespread. Channeling the\nconcerns and fears of our interlocutors, we call for more research on how\nactivists can protect themselves against evermore fine-grained location data\ntracking.",
      "link": "http://arxiv.org/abs/2501.16885v1",
      "categories": [
        "cs.HC",
        "cs.CY"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "Non-Western Perspectives on Web Inclusivity: A Study of Accessibility\n  Practices in the Global South",
      "authors": [
        "Masudul Hasan Masud Bhuiyan",
        "Matteo Varvello",
        "Cristian-Alexandru Staicu",
        "Yasir Zaki"
      ],
      "published": "2025-01-28T00:30:03Z",
      "summary": "The Global South faces unique challenges in achieving digital inclusion due\nto a heavy reliance on mobile devices for internet access and the prevalence of\nslow or unreliable networks. While numerous studies have investigated web\naccessibility within specific sectors such as education, healthcare, and\ngovernment services, these efforts have been largely constrained to individual\ncountries or narrow contexts, leaving a critical gap in cross-regional,\nlarge-scale analysis. This paper addresses this gap by conducting the first\nlarge-scale comparative study of mobile web accessibility across the Global\nSouth. In this work, we evaluate 100,000 websites from 10 countries in the\nGlobal South to provide a comprehensive understanding of accessibility\npractices in these regions. Our findings reveal that websites from countries\nwith strict accessibility regulations and enforcement tend to adhere better to\nWeb Content Accessibility Guidelines (WCAG) guidelines. However, accessibility\nviolations impact different disability groups in varying ways. Blind and\nlow-vision individuals in the Global South are disproportionately affected, as\nonly 40% of the evaluated websites meet critical accessibility guidelines. This\nsignificant shortfall is largely due to developers frequently neglecting to\nimplement valid alt text for images and ARIA descriptions, which are essential\nspecification mechanisms in the HTML standard for the effective operation of\nscreen readers.",
      "link": "http://arxiv.org/abs/2501.16601v1",
      "categories": [
        "cs.SE",
        "cs.CY",
        "cs.HC"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "Blissful (A)Ignorance: People form overly positive impressions of others\n  based on their written messages, despite wide-scale adoption of Generative AI",
      "authors": [
        "Jiaqi Zhu",
        "Andras Molnar"
      ],
      "published": "2025-01-26T21:38:12Z",
      "summary": "As the use of Generative AI (GenAI) tools becomes more prevalent in\ninterpersonal communication, understanding their impact on social perceptions\nis crucial. According to signaling theory, GenAI may undermine the credibility\nof social signals conveyed in writing, since it reduces the cost of writing and\nmakes it hard to verify the authenticity of messages. Using a pre-registered\nlarge-scale online experiment (N = 647; Prolific), featuring scenarios in a\nrange of communication contexts (personal vs. professional; close others vs.\nstrangers), we explored how senders' use of GenAI influenced recipients'\nimpressions of senders, both when GenAI use was known or uncertain. Consistent\nwith past work, we found strong negative effects on social impressions when\ndisclosing that a message was AI-generated, compared to when the same message\nwas human-written. However, under the more realistic condition when potential\nGenAI use was not explicitly highlighted, recipients did not exhibit any\nskepticism towards senders, and these \"uninformed\" impressions were virtually\nindistinguishable from those of fully human-written messages. Even when we\nhighlighted the potential (but uncertain) use of GenAI, recipients formed\noverly positive impressions. These results are especially striking given that\n46% of our sample admitted having used such tools for writing messages, just\nwithin the past two weeks. Our findings put past work in a new light: While\nsocial judgments can be substantially affected when GenAI use is explicitly\ndisclosed, this information may not be readily available in more realistic\ncommunication settings, making recipients blissfully ignorant about others'\npotential use of GenAI.",
      "link": "http://arxiv.org/abs/2501.15678v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "combination": "cs.CY AND cs.HC"
    },
    {
      "title": "Dialogue Systems for Emotional Support via Value Reinforcement",
      "authors": [
        "Juhee Kim",
        "Chunghu Mok",
        "Jisun Lee",
        "Hyang Sook Kim",
        "Yohan Jo"
      ],
      "published": "2025-01-25T11:51:31Z",
      "summary": "Emotional support dialogue systems aim to reduce help-seekers' distress and\nhelp them overcome challenges. While human values$\\unicode{x2013}$core beliefs\nthat shape an individual's priorities$\\unicode{x2013}$are increasingly\nemphasized in contemporary psychological therapy for their role in fostering\ninternal transformation and long-term emotional well-being, their integration\ninto emotional support systems remains underexplored. To bridge this gap, we\npresent a value-driven method for training emotional support dialogue systems\ndesigned to reinforce positive values in seekers. Our model learns to identify\nwhich values to reinforce at each turn and how to do so, by leveraging online\nsupport conversations from Reddit. The model demonstrated superior performance\nin emotional support capabilities, outperforming various baselines. Notably, it\nmore effectively explored and elicited values from seekers. Expert assessments\nby therapists highlighted two key strengths of our model: its ability to\nvalidate users' challenges and its effectiveness in emphasizing positive\naspects of their situations$\\unicode{x2013}$both crucial elements of value\nreinforcement. Our work validates the effectiveness of value reinforcement for\nemotional support systems and establishes a foundation for future research.",
      "link": "http://arxiv.org/abs/2501.17182v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "I.2.7"
      ],
      "combination": "cs.CY AND cs.HC"
    }
  ]
}