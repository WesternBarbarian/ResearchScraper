{"papers_7_2025-01-26": {"timestamp": 1737921473.706369, "data": [{"title": "Perceived Fairness of the Machine Learning Development Process: Concept\n  Scale Development", "authors": ["Anoop Mishra", "Deepak Khazanchi"], "published": "2025-01-23T06:51:31Z", "summary": "In machine learning (ML) applications, unfairness is triggered due to bias in\nthe data, the data curation process, erroneous assumptions, and implicit bias\nrendered during the development process. It is also well-accepted by\nresearchers that fairness in ML application development is highly subjective,\nwith a lack of clarity of what it means from an ML development and\nimplementation perspective. Thus, in this research, we investigate and\nformalize the notion of the perceived fairness of ML development from a\nsociotechnical lens. Our goal in this research is to understand the\ncharacteristics of perceived fairness in ML applications. We address this\nresearch goal using a three-pronged strategy: 1) conducting virtual focus\ngroups with ML developers, 2) reviewing existing literature on fairness in ML,\nand 3) incorporating aspects of justice theory relating to procedural and\ndistributive justice. Based on our theoretical exposition, we propose\noperational attributes of perceived fairness to be transparency,\naccountability, and representativeness. These are described in terms of\nmultiple concepts that comprise each dimension of perceived fairness. We use\nthis operationalization to empirically validate the notion of perceived\nfairness of machine learning (ML) applications from both the ML practioners and\nusers perspectives. The multidimensional framework for perceived fairness\noffers a comprehensive understanding of perceived fairness, which can guide the\ncreation of fair ML systems with positive implications for society and\nbusinesses.", "link": "http://arxiv.org/abs/2501.13421v1", "categories": ["cs.HC", "cs.CY", "cs.LG", "J.4; J.1; K.4; K.6; I.2; E.m"], "combination": "cs.CY AND cs.HC"}, {"title": "Development of the Critical Reflection and Agency in Computing Index", "authors": ["Aadarsh Padiyath", "Mark Guzdial", "Barbara Ericson"], "published": "2025-01-22T18:13:05Z", "summary": "As computing's societal impact grows, so does the need for computing students\nto recognize and address the ethical and sociotechnical implications of their\nwork. While there are efforts to integrate ethics into computing curricula, we\nlack a standardized tool to measure those efforts, specifically, students'\nattitudes towards ethical reflection and their ability to effect change. This\npaper introduces the novel framework of Critically Conscious Computing and\nreports on the development and content validation of the Critical Reflection\nand Agency in Computing Index, a novel instrument designed to assess\nundergraduate computing students' attitudes towards practicing critically\nconscious computing. The resulting index is a theoretically grounded,\nexpert-reviewed tool to support research and practice in computing ethics\neducation. This enables researchers and educators to gain insights into\nstudents' perspectives, inform the design of targeted ethics interventions, and\nmeasure the effectiveness of computing ethics education initiatives.", "link": "http://arxiv.org/abs/2501.13060v1", "categories": ["cs.CY", "cs.HC"], "combination": "cs.CY AND cs.HC"}, {"title": "Designing and Evaluating an Educational Recommender System with\n  Different Levels of User Control", "authors": ["Qurat Ul Ain", "Mohamed Amine Chatti", "William Kana Tsoplefack", "Rawaa Alatrash", "Shoeb Joarder"], "published": "2025-01-22T14:14:49Z", "summary": "Educational recommender systems (ERSs) play a crucial role in personalizing\nlearning experiences and enhancing educational outcomes by providing\nrecommendations of personalized resources and activities to learners, tailored\nto their individual learning needs. However, their effectiveness is often\ndiminished by insufficient user control and limited transparency. To address\nthese challenges, in this paper, we present the systematic design and\nevaluation of an interactive ERS, in which we introduce different levels of\nuser control. Concretely, we introduce user control around the input (i.e.,\nuser profile), process (i.e., recommendation algorithm), and output (i.e.,\nrecommendations) of the ERS. To evaluate our system, we conducted an online\nuser study (N=30) to explore the impact of user control on users' perceptions\nof the ERS in terms of several important user-centric aspects. Moreover, we\ninvestigated the effects of user control on multiple recommendation goals,\nnamely transparency, trust, and satisfaction, as well as the interactions\nbetween these goals. Our results demonstrate the positive impact of user\ncontrol on user perceived benefits of the ERS. Moreover, our study shows that\nuser control strongly correlates with transparency and moderately correlates\nwith trust and satisfaction. In terms of interaction between these goals, our\nresults reveal that transparency moderately correlates and trust strongly\ncorrelates with satisfaction. Whereas, transparency and trust stand out as less\ncorrelated with each other.", "link": "http://arxiv.org/abs/2501.12894v1", "categories": ["cs.IR", "cs.CY", "cs.HC"], "combination": "cs.CY AND cs.HC"}, {"title": "Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at\n  CHI through a Systematic Literature Review", "authors": ["Rock Yuren Pang", "Hope Schroeder", "Kynnedy Simone Smith", "Solon Barocas", "Ziang Xiao", "Emily Tseng", "Danielle Bragg"], "published": "2025-01-22T00:31:51Z", "summary": "Large language models (LLMs) have been positioned to revolutionize HCI, by\nreshaping not only the interfaces, design patterns, and sociotechnical systems\nthat we study, but also the research practices we use. To-date, however, there\nhas been little understanding of LLMs' uptake in HCI. We address this gap via a\nsystematic literature review of 153 CHI papers from 2020-24 that engage with\nLLMs. We taxonomize: (1) domains where LLMs are applied; (2) roles of LLMs in\nHCI projects; (3) contribution types; and (4) acknowledged limitations and\nrisks. We find LLM work in 10 diverse domains, primarily via empirical and\nartifact contributions. Authors use LLMs in five distinct roles, including as\nresearch tools or simulated users. Still, authors often raise validity and\nreproducibility concerns, and overwhelmingly study closed models. We outline\nopportunities to improve HCI research with and on LLMs, and provide guiding\nquestions for researchers to consider the validity and appropriateness of\nLLM-related work.", "link": "http://arxiv.org/abs/2501.12557v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "combination": "cs.CY AND cs.HC"}, {"title": "A Basis for Human Responsibility in Artificial Intelligence Computation", "authors": ["Vincenzo Calderonio"], "published": "2025-01-21T20:59:48Z", "summary": "Recent advancements in artificial intelligence have reopened the question\nabout the boundaries of AI autonomy, particularly in discussions around\nartificial general intelligence (AGI) and its potential to act independently\nacross varied purposes. This paper explores these boundaries through the\nanalysis of the Alignment Research Center experiment on GPT-4 and introduces\nthe Start Button Problem, a thought experiment that examines the origins and\nlimits of AI autonomy. By examining the thought experiment and its\ncounterarguments will be enlightened how in the need for human activation and\npurpose definition lies the AI's inherent dependency on human-initiated\nactions, challenging the assumption of AI as an agent. Finally, the paper\naddresses the implications of this dependency on human responsibility,\nquestioning the measure of the extension of human responsibility when using AI\nsystems.", "link": "http://arxiv.org/abs/2501.12498v1", "categories": ["cs.CY", "cs.HC", "F.0; I.2; K.4; K.5"], "combination": "cs.CY AND cs.HC"}, {"title": "Expertise elevates AI usage: experimental evidence comparing laypeople\n  and professional artists", "authors": ["Thomas F. Eisenmann", "Andres Karjus", "Mar Canet Sola", "Levin Brinkmann", "Bramantyo Ibrahim Supriyatno", "Iyad Rahwan"], "published": "2025-01-21T18:53:21Z", "summary": "Novel capacities of generative AI to analyze and generate cultural artifacts\nraise inevitable questions about the nature and value of artistic education and\nhuman expertise. Has AI already leveled the playing field between professional\nartists and laypeople, or do trained artistic expressive capacity, curation\nskills and experience instead enhance the ability to use these new tools? In\nthis pre-registered study, we conduct experimental comparisons between 50\nactive artists and a demographically matched sample of laypeople. We designed\ntwo tasks to approximate artistic practice for testing their capabilities in\nboth faithful and creative image creation: replicating a reference image, and\nmoving as far away as possible from it. We developed a bespoke platform where\nparticipants used a modern text-to-image model to complete both tasks. We also\ncollected and compared participants' sentiments towards AI. On average, artists\nproduced more faithful and creative outputs than their lay counterparts,\nalthough only by a small margin. While AI may ease content creation,\nprofessional expertise is still valuable - even within the confined space of\ngenerative AI itself. Finally, we also explored how well an exemplary\nvision-capable large language model (GPT-4o) would complete the same tasks, if\ngiven the role of an image generation agent, and found it performed on par in\ncopying but outperformed even artists in the creative task. The very best\nresults were still produced by humans in both tasks. These outcomes highlight\nthe importance of integrating artistic skills with AI training to prepare\nartists and other visual professionals for a technologically evolving\nlandscape. We see a potential in collaborative synergy with generative AI,\nwhich could reshape creative industries and education in the arts.", "link": "http://arxiv.org/abs/2501.12374v1", "categories": ["cs.HC", "cs.AI", "cs.CY"], "combination": "cs.CY AND cs.HC"}]}, "papers_1_2025-01-28": {"timestamp": 1738033711.3879905, "data": []}, "papers_2_2025-01-28": {"timestamp": 1738033721.2908695, "data": []}, "papers_10_2025-01-28": {"timestamp": 1738033727.547476, "data": [{"title": "Design and Implementation of a Psychiatry Resident Training System Based\n  on Large Language Models", "authors": ["Zhenguang Zhong", "Jia Tang"], "published": "2025-01-24T14:35:21Z", "summary": "Mental disorders have become a significant global public health issue, while\nthe shortage of psychiatrists and inefficient training systems severely hinder\nthe accessibility of mental health services. This paper designs and implements\nan artificial intelligence-based training system for psychiatrists. By\nintegrating technologies such as large language models, knowledge graphs, and\nexpert systems, the system constructs an intelligent and standardized training\nplatform. It includes six functional modules: case generation, consultation\ndialogue, examination prescription, diagnostic decision-making, integrated\ntraditional Chinese and Western medicine prescription, and expert evaluation,\nproviding comprehensive support from clinical skill training to professional\nlevel assessment.The system adopts a B/S architecture, developed using the\nVue.js and Node.js technology stack, and innovatively applies deep learning\nalgorithms for case generation and doctor-patient dialogue. In a clinical trial\ninvolving 60 psychiatrists at different levels, the system demonstrated\nexcellent performance and training outcomes: system stability reached 99.95%,\nAI dialogue accuracy achieved 96.5%, diagnostic accuracy reached 92.5%, and\nuser satisfaction scored 92.3%. Experimental data showed that doctors using the\nsystem improved their knowledge mastery, clinical thinking, and diagnostic\nskills by 35.6%, 28.4%, and 23.7%, respectively.The research results provide an\ninnovative solution for improving the efficiency of psychiatrist training and\nhold significant importance for promoting the standardization and scalability\nof mental health professional development.", "link": "http://arxiv.org/abs/2501.14530v1", "categories": ["cs.CY", "cs.HC", "q-bio.NC"], "combination": "cs.CY AND cs.HC"}, {"title": "Reddit Rules and Rulers: Quantifying the Link Between Rules and\n  Perceptions of Governance across Thousands of Communities", "authors": ["Leon Leibmann", "Galen Weld", "Amy X. Zhang", "Tim Althoff"], "published": "2025-01-24T01:26:41Z", "summary": "Rules are a critical component of the functioning of nearly every online\ncommunity, yet it is challenging for community moderators to make data-driven\ndecisions about what rules to set for their communities. The connection between\na community's rules and how its membership feels about its governance is not\nwell understood. In this work, we conduct the largest-to-date analysis of rules\non Reddit, collecting a set of 67,545 unique rules across 5,225 communities\nwhich collectively account for more than 67% of all content on Reddit. More\nthan just a point-in-time study, our work measures how communities change their\nrules over a 5+ year period. We develop a method to classify these rules using\na taxonomy of 17 key attributes extended from previous work. We assess what\ntypes of rules are most prevalent, how rules are phrased, and how they vary\nacross communities of different types. Using a dataset of communities'\ndiscussions about their governance, we are the first to identify the rules most\nstrongly associated with positive community perceptions of governance: rules\naddressing who participates, how content is formatted and tagged, and rules\nabout commercial activities. We conduct a longitudinal study to quantify the\nimpact of adding new rules to communities, finding that after a rule is added,\ncommunity perceptions of governance immediately improve, yet this effect\ndiminishes after six months. Our results have important implications for\nplatforms, moderators, and researchers. We make our classification model and\nrules datasets public to support future research on this topic.", "link": "http://arxiv.org/abs/2501.14163v1", "categories": ["cs.SI", "cs.CY", "cs.HC"], "combination": "cs.CY AND cs.HC"}, {"title": "Exploring User Perspectives on Data Collection, Data Sharing\n  Preferences, and Privacy Concerns with Remote Healthcare Technology", "authors": ["Daniela Napoli", "Heather Molyneaux", "Helene Fournier", "Sonia Chiasson"], "published": "2025-01-23T21:09:03Z", "summary": "Remote healthcare technology can help tackle societal issues by improving\naccess to quality healthcare services and enhancing diagnoses through in-place\nmonitoring. These services can be implemented through a combination of mobile\ndevices, applications, wearable sensors, and other smart technology. It is\nparamount to handle sensitive data that is collected in ways that meet users'\nprivacy expectations. We surveyed 384 people in Canada aged 20 to 93 years old\nto explore participants' comfort with data collection, sharing preferences, and\npotential privacy concerns related to remote healthcare technology. We explore\nthese topics within the context of various healthcare scenarios including\nhealth emergencies and managing chronic health conditions.", "link": "http://arxiv.org/abs/2501.14098v1", "categories": ["cs.CY", "cs.HC"], "combination": "cs.CY AND cs.HC"}, {"title": "The Role of Generative AI in Software Student CollaborAItion", "authors": ["Natalie Kiesler", "Jacqueline Smith", "Juho Leinonen", "Armando Fox", "Stephen MacNeil", "Petri Ihantola"], "published": "2025-01-23T20:43:05Z", "summary": "Collaboration is a crucial part of computing education. The increase in AI\ncapabilities over the last couple of years is bound to profoundly affect all\naspects of systems and software engineering, including collaboration. In this\nposition paper, we consider a scenario where AI agents would be able to take on\nany role in collaborative processes in computing education. We outline these\nroles, the activities and group dynamics that software development currently\ninclude, and discuss if and in what way AI could facilitate these roles and\nactivities. The goal of our work is to envision and critically examine\npotential futures. We present scenarios suggesting how AI can be integrated\ninto existing collaborations. These are contrasted by design fictions that help\ndemonstrate the new possibilities and challenges for computing education in the\nAI era.", "link": "http://arxiv.org/abs/2501.14084v1", "categories": ["cs.SE", "cs.AI", "cs.CY", "cs.HC"], "combination": "cs.CY AND cs.HC"}, {"title": "Perceived Fairness of the Machine Learning Development Process: Concept\n  Scale Development", "authors": ["Anoop Mishra", "Deepak Khazanchi"], "published": "2025-01-23T06:51:31Z", "summary": "In machine learning (ML) applications, unfairness is triggered due to bias in\nthe data, the data curation process, erroneous assumptions, and implicit bias\nrendered during the development process. It is also well-accepted by\nresearchers that fairness in ML application development is highly subjective,\nwith a lack of clarity of what it means from an ML development and\nimplementation perspective. Thus, in this research, we investigate and\nformalize the notion of the perceived fairness of ML development from a\nsociotechnical lens. Our goal in this research is to understand the\ncharacteristics of perceived fairness in ML applications. We address this\nresearch goal using a three-pronged strategy: 1) conducting virtual focus\ngroups with ML developers, 2) reviewing existing literature on fairness in ML,\nand 3) incorporating aspects of justice theory relating to procedural and\ndistributive justice. Based on our theoretical exposition, we propose\noperational attributes of perceived fairness to be transparency,\naccountability, and representativeness. These are described in terms of\nmultiple concepts that comprise each dimension of perceived fairness. We use\nthis operationalization to empirically validate the notion of perceived\nfairness of machine learning (ML) applications from both the ML practioners and\nusers perspectives. The multidimensional framework for perceived fairness\noffers a comprehensive understanding of perceived fairness, which can guide the\ncreation of fair ML systems with positive implications for society and\nbusinesses.", "link": "http://arxiv.org/abs/2501.13421v1", "categories": ["cs.HC", "cs.CY", "cs.LG", "J.4; J.1; K.4; K.6; I.2; E.m"], "combination": "cs.CY AND cs.HC"}, {"title": "Development of the Critical Reflection and Agency in Computing Index", "authors": ["Aadarsh Padiyath", "Mark Guzdial", "Barbara Ericson"], "published": "2025-01-22T18:13:05Z", "summary": "As computing's societal impact grows, so does the need for computing students\nto recognize and address the ethical and sociotechnical implications of their\nwork. While there are efforts to integrate ethics into computing curricula, we\nlack a standardized tool to measure those efforts, specifically, students'\nattitudes towards ethical reflection and their ability to effect change. This\npaper introduces the novel framework of Critically Conscious Computing and\nreports on the development and content validation of the Critical Reflection\nand Agency in Computing Index, a novel instrument designed to assess\nundergraduate computing students' attitudes towards practicing critically\nconscious computing. The resulting index is a theoretically grounded,\nexpert-reviewed tool to support research and practice in computing ethics\neducation. This enables researchers and educators to gain insights into\nstudents' perspectives, inform the design of targeted ethics interventions, and\nmeasure the effectiveness of computing ethics education initiatives.", "link": "http://arxiv.org/abs/2501.13060v1", "categories": ["cs.CY", "cs.HC"], "combination": "cs.CY AND cs.HC"}, {"title": "Designing and Evaluating an Educational Recommender System with\n  Different Levels of User Control", "authors": ["Qurat Ul Ain", "Mohamed Amine Chatti", "William Kana Tsoplefack", "Rawaa Alatrash", "Shoeb Joarder"], "published": "2025-01-22T14:14:49Z", "summary": "Educational recommender systems (ERSs) play a crucial role in personalizing\nlearning experiences and enhancing educational outcomes by providing\nrecommendations of personalized resources and activities to learners, tailored\nto their individual learning needs. However, their effectiveness is often\ndiminished by insufficient user control and limited transparency. To address\nthese challenges, in this paper, we present the systematic design and\nevaluation of an interactive ERS, in which we introduce different levels of\nuser control. Concretely, we introduce user control around the input (i.e.,\nuser profile), process (i.e., recommendation algorithm), and output (i.e.,\nrecommendations) of the ERS. To evaluate our system, we conducted an online\nuser study (N=30) to explore the impact of user control on users' perceptions\nof the ERS in terms of several important user-centric aspects. Moreover, we\ninvestigated the effects of user control on multiple recommendation goals,\nnamely transparency, trust, and satisfaction, as well as the interactions\nbetween these goals. Our results demonstrate the positive impact of user\ncontrol on user perceived benefits of the ERS. Moreover, our study shows that\nuser control strongly correlates with transparency and moderately correlates\nwith trust and satisfaction. In terms of interaction between these goals, our\nresults reveal that transparency moderately correlates and trust strongly\ncorrelates with satisfaction. Whereas, transparency and trust stand out as less\ncorrelated with each other.", "link": "http://arxiv.org/abs/2501.12894v1", "categories": ["cs.IR", "cs.CY", "cs.HC"], "combination": "cs.CY AND cs.HC"}, {"title": "Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at\n  CHI through a Systematic Literature Review", "authors": ["Rock Yuren Pang", "Hope Schroeder", "Kynnedy Simone Smith", "Solon Barocas", "Ziang Xiao", "Emily Tseng", "Danielle Bragg"], "published": "2025-01-22T00:31:51Z", "summary": "Large language models (LLMs) have been positioned to revolutionize HCI, by\nreshaping not only the interfaces, design patterns, and sociotechnical systems\nthat we study, but also the research practices we use. To-date, however, there\nhas been little understanding of LLMs' uptake in HCI. We address this gap via a\nsystematic literature review of 153 CHI papers from 2020-24 that engage with\nLLMs. We taxonomize: (1) domains where LLMs are applied; (2) roles of LLMs in\nHCI projects; (3) contribution types; and (4) acknowledged limitations and\nrisks. We find LLM work in 10 diverse domains, primarily via empirical and\nartifact contributions. Authors use LLMs in five distinct roles, including as\nresearch tools or simulated users. Still, authors often raise validity and\nreproducibility concerns, and overwhelmingly study closed models. We outline\nopportunities to improve HCI research with and on LLMs, and provide guiding\nquestions for researchers to consider the validity and appropriateness of\nLLM-related work.", "link": "http://arxiv.org/abs/2501.12557v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "combination": "cs.CY AND cs.HC"}, {"title": "A Basis for Human Responsibility in Artificial Intelligence Computation", "authors": ["Vincenzo Calderonio"], "published": "2025-01-21T20:59:48Z", "summary": "Recent advancements in artificial intelligence have reopened the question\nabout the boundaries of AI autonomy, particularly in discussions around\nartificial general intelligence (AGI) and its potential to act independently\nacross varied purposes. This paper explores these boundaries through the\nanalysis of the Alignment Research Center experiment on GPT-4 and introduces\nthe Start Button Problem, a thought experiment that examines the origins and\nlimits of AI autonomy. By examining the thought experiment and its\ncounterarguments will be enlightened how in the need for human activation and\npurpose definition lies the AI's inherent dependency on human-initiated\nactions, challenging the assumption of AI as an agent. Finally, the paper\naddresses the implications of this dependency on human responsibility,\nquestioning the measure of the extension of human responsibility when using AI\nsystems.", "link": "http://arxiv.org/abs/2501.12498v1", "categories": ["cs.CY", "cs.HC", "F.0; I.2; K.4; K.5"], "combination": "cs.CY AND cs.HC"}, {"title": "Expertise elevates AI usage: experimental evidence comparing laypeople\n  and professional artists", "authors": ["Thomas F. Eisenmann", "Andres Karjus", "Mar Canet Sola", "Levin Brinkmann", "Bramantyo Ibrahim Supriyatno", "Iyad Rahwan"], "published": "2025-01-21T18:53:21Z", "summary": "Novel capacities of generative AI to analyze and generate cultural artifacts\nraise inevitable questions about the nature and value of artistic education and\nhuman expertise. Has AI already leveled the playing field between professional\nartists and laypeople, or do trained artistic expressive capacity, curation\nskills and experience instead enhance the ability to use these new tools? In\nthis pre-registered study, we conduct experimental comparisons between 50\nactive artists and a demographically matched sample of laypeople. We designed\ntwo tasks to approximate artistic practice for testing their capabilities in\nboth faithful and creative image creation: replicating a reference image, and\nmoving as far away as possible from it. We developed a bespoke platform where\nparticipants used a modern text-to-image model to complete both tasks. We also\ncollected and compared participants' sentiments towards AI. On average, artists\nproduced more faithful and creative outputs than their lay counterparts,\nalthough only by a small margin. While AI may ease content creation,\nprofessional expertise is still valuable - even within the confined space of\ngenerative AI itself. Finally, we also explored how well an exemplary\nvision-capable large language model (GPT-4o) would complete the same tasks, if\ngiven the role of an image generation agent, and found it performed on par in\ncopying but outperformed even artists in the creative task. The very best\nresults were still produced by humans in both tasks. These outcomes highlight\nthe importance of integrating artistic skills with AI training to prepare\nartists and other visual professionals for a technologically evolving\nlandscape. We see a potential in collaborative synergy with generative AI,\nwhich could reshape creative industries and education in the arts.", "link": "http://arxiv.org/abs/2501.12374v1", "categories": ["cs.HC", "cs.AI", "cs.CY"], "combination": "cs.CY AND cs.HC"}]}}