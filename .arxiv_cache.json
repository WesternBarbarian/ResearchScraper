{"papers_7_2025-01-26": {"timestamp": 1737914307.070073, "data": [{"title": "Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass", "authors": ["Jianing Yang", "Alexander Sax", "Kevin J. Liang", "Mikael Henaff", "Hao Tang", "Ang Cao", "Joyce Chai", "Franziska Meier", "Matt Feiszli"], "published": "2025-01-23T18:59:55Z", "summary": "Multi-view 3D reconstruction remains a core challenge in computer vision,\nparticularly in applications requiring accurate and scalable representations\nacross diverse perspectives. Current leading methods such as DUSt3R employ a\nfundamentally pairwise approach, processing images in pairs and necessitating\ncostly global alignment procedures to reconstruct from multiple views. In this\nwork, we propose Fast 3D Reconstruction (Fast3R), a novel multi-view\ngeneralization to DUSt3R that achieves efficient and scalable 3D reconstruction\nby processing many views in parallel. Fast3R's Transformer-based architecture\nforwards N images in a single forward pass, bypassing the need for iterative\nalignment. Through extensive experiments on camera pose estimation and 3D\nreconstruction, Fast3R demonstrates state-of-the-art performance, with\nsignificant improvements in inference speed and reduced error accumulation.\nThese results establish Fast3R as a robust alternative for multi-view\napplications, offering enhanced scalability without compromising reconstruction\naccuracy.", "link": "http://arxiv.org/abs/2501.13928v1", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.RO"]}, {"title": "CRPO: Confidence-Reward Driven Preference Optimization for Machine\n  Translation", "authors": ["Guofeng Cui", "Pichao Wang", "Yang Liu", "Zemian Ke", "Zhu Liu", "Vimal Bhat"], "published": "2025-01-23T18:59:47Z", "summary": "Large language models (LLMs) have shown great potential in natural language\nprocessing tasks, but their application to machine translation (MT) remains\nchallenging due to pretraining on English-centric data and the complexity of\nreinforcement learning from human feedback (RLHF). Direct Preference\nOptimization (DPO) has emerged as a simpler and more efficient alternative, but\nits performance depends heavily on the quality of preference data. To address\nthis, we propose Confidence-Reward driven Preference Optimization (CRPO), a\nnovel method that combines reward scores with model confidence to improve data\nselection for fine-tuning. CRPO selects challenging sentence pairs where the\nmodel is uncertain or underperforms, leading to more effective learning. While\nprimarily designed for LLMs, CRPO also generalizes to encoder-decoder models\nlike NLLB, demonstrating its versatility. Empirical results show that CRPO\noutperforms existing methods such as RS-DPO, RSO and MBR score in both\ntranslation accuracy and data efficiency.", "link": "http://arxiv.org/abs/2501.13927v1", "categories": ["cs.CL", "cs.AI", "cs.CV"]}, {"title": "Can We Generate Images with CoT? Let's Verify and Reinforce Image\n  Generation Step by Step", "authors": ["Ziyu Guo", "Renrui Zhang", "Chengzhuo Tong", "Zhizheng Zhao", "Peng Gao", "Hongsheng Li", "Pheng-Ann Heng"], "published": "2025-01-23T18:59:43Z", "summary": "Chain-of-Thought (CoT) reasoning has been extensively explored in large\nmodels to tackle complex understanding tasks. However, it still remains an open\nquestion whether such strategies can be applied to verifying and reinforcing\nimage generation scenarios. In this paper, we provide the first comprehensive\ninvestigation of the potential of CoT reasoning to enhance autoregressive image\ngeneration. We focus on three techniques: scaling test-time computation for\nverification, aligning model preferences with Direct Preference Optimization\n(DPO), and integrating these techniques for complementary effects. Our results\ndemonstrate that these approaches can be effectively adapted and combined to\nsignificantly improve image generation performance. Furthermore, given the\npivotal role of reward models in our findings, we propose the Potential\nAssessment Reward Model (PARM) and PARM++, specialized for autoregressive image\ngeneration. PARM adaptively assesses each generation step through a potential\nassessment approach, merging the strengths of existing reward models, and\nPARM++ further introduces a reflection mechanism to self-correct the generated\nunsatisfactory image. Using our investigated reasoning strategies, we enhance a\nbaseline model, Show-o, to achieve superior results, with a significant +24%\nimprovement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We\nhope our study provides unique insights and paves a new path for integrating\nCoT reasoning with autoregressive image generation. Code and models are\nreleased at https://github.com/ZiyuGuo99/Image-Generation-CoT", "link": "http://arxiv.org/abs/2501.13926v1", "categories": ["cs.CV", "cs.AI", "cs.CL"]}, {"title": "Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive\n  Entropy-aware Optimization", "authors": ["Hao Dong", "Eleni Chatzi", "Olga Fink"], "published": "2025-01-23T18:59:30Z", "summary": "Test-time adaptation (TTA) has demonstrated significant potential in\naddressing distribution shifts between training and testing data. Open-set\ntest-time adaptation (OSTTA) aims to adapt a source pre-trained model online to\nan unlabeled target domain that contains unknown classes. This task becomes\nmore challenging when multiple modalities are involved. Existing methods have\nprimarily focused on unimodal OSTTA, often filtering out low-confidence samples\nwithout addressing the complexities of multimodal data. In this work, we\npresent Adaptive Entropy-aware Optimization (AEO), a novel framework\nspecifically designed to tackle Multimodal Open-set Test-time Adaptation\n(MM-OSTTA) for the first time. Our analysis shows that the entropy difference\nbetween known and unknown samples in the target domain strongly correlates with\nMM-OSTTA performance. To leverage this, we propose two key components:\nUnknown-aware Adaptive Entropy Optimization (UAE) and Adaptive Modality\nPrediction Discrepancy Optimization (AMP). These components enhance the ability\nof model to distinguish unknown class samples during online adaptation by\namplifying the entropy difference between known and unknown samples. To\nthoroughly evaluate our proposed methods in the MM-OSTTA setting, we establish\na new benchmark derived from existing datasets. This benchmark includes two\ndownstream tasks and incorporates five modalities. Extensive experiments across\nvarious domain shift situations demonstrate the efficacy and versatility of the\nAEO framework. Additionally, we highlight the strong performance of AEO in\nlong-term and continual MM-OSTTA settings, both of which are challenging and\nhighly relevant to real-world applications. Our source code is available at\nhttps://github.com/donghao51/AEO.", "link": "http://arxiv.org/abs/2501.13924v1", "categories": ["cs.CV", "cs.AI", "cs.LG"]}, {"title": "GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing", "authors": ["Akashah Shabbir", "Mohammed Zumri", "Mohammed Bennamoun", "Fahad S. Khan", "Salman Khan"], "published": "2025-01-23T18:59:30Z", "summary": "Recent advances in large multimodal models (LMMs) have recognized\nfine-grained grounding as an imperative factor of visual understanding and\ndialogue. However, the benefits of such representation in LMMs are limited to\nthe natural image domain, and these models perform poorly for remote sensing\n(RS). The distinct overhead viewpoint, scale variation, and presence of small\nobjects in high-resolution RS imagery present a unique challenge in\nregion-level comprehension. Moreover, the development of the grounding\nconversation capability of LMMs within RS is hindered by the lack of granular,\nRS domain-specific grounded data. Addressing these limitations, we propose\nGeoPixel - the first end-to-end high resolution RS-LMM that supports\npixel-level grounding. This capability allows fine-grained visual perception by\ngenerating interleaved masks in conversation. GeoPixel supports up to 4K HD\nresolution in any aspect ratio, ideal for high-precision RS image analysis. To\nsupport the grounded conversation generation (GCG) in RS imagery, we curate a\nvisually grounded dataset GeoPixelD through a semi-automated pipeline that\nutilizes set-of-marks prompting and spatial priors tailored for RS data to\nmethodically control the data generation process. GeoPixel demonstrates\nsuperior performance in pixel-level comprehension, surpassing existing LMMs in\nboth single-target and multi-target segmentation tasks. Our methodological\nablation studies validate the effectiveness of each component in the overall\narchitecture. Our code and data will be publicly released.", "link": "http://arxiv.org/abs/2501.13925v1", "categories": ["cs.CV"]}, {"title": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities", "authors": ["Chan-Jan Hsu", "Chia-Sheng Liu", "Meng-Hsi Chen", "Muxi Chen", "Po-Chun Hsu", "Yi-Chang Chen", "Da-Shan Shiu"], "published": "2025-01-23T18:59:02Z", "summary": "Breeze 2 is a suite of advanced multi-modal language models, available in 3B\nand 8B parameter configurations, specifically designed to enhance Traditional\nChinese language representation. Building upon the Llama 3, Breeze 2 continues\npretraining on an extensive corpus to enhance the linguistic and cultural\nheritage of Traditional Chinese. It incorporates vision-aware capabilities\nthrough a visual encoder and a bridge module, and supports function-calling via\nprompt templates and post-training on function-calling data. The effectiveness\nof Breeze 2 is benchmarked across various tasks, including Taiwan general\nknowledge, instruction-following, long context, function calling, and vision\nunderstanding. Furthermore, we showcase the capabilities of the its 3B model in\na mobile application. We are publicly releasing all Breeze 2 models under the\nLlama 3 Community License.", "link": "http://arxiv.org/abs/2501.13921v1", "categories": ["cs.CL"]}, {"title": "IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art\n  Text-to-Image Models", "authors": ["Jiayi Lei", "Renrui Zhang", "Xiangfei Hu", "Weifeng Lin", "Zhen Li", "Wenjian Sun", "Ruoyi Du", "Le Zhuo", "Zhongyu Li", "Xinyue Li", "Shitian Zhao", "Ziyu Guo", "Yiting Lu", "Peng Gao", "Hongsheng Li"], "published": "2025-01-23T18:58:33Z", "summary": "With the rapid development of diffusion models, text-to-image(T2I) models\nhave made significant progress, showcasing impressive abilities in prompt\nfollowing and image generation. Recently launched models such as FLUX.1 and\nIdeogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have\ndemonstrated exceptional performance across various complex tasks, raising\nquestions about whether T2I models are moving towards general-purpose\napplicability. Beyond traditional image generation, these models exhibit\ncapabilities across a range of fields, including controllable generation, image\nediting, video, audio, 3D, and motion generation, as well as computer vision\ntasks like semantic segmentation and depth estimation. However, current\nevaluation frameworks are insufficient to comprehensively assess these models'\nperformance across expanding domains. To thoroughly evaluate these models, we\ndeveloped the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0,\nMidjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided\ninto five key domains: structured output generation, realism, and physical\nconsistency, specific domain generation, challenging scenario generation, and\nmulti-style creation tasks. This comprehensive assessment highlights each\nmodel's strengths and limitations, particularly the outstanding performance of\nFLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring\nthe expanding applications and potential of T2I models as foundational AI\ntools. This study provides valuable insights into the current state and future\ntrajectory of T2I models as they evolve towards general-purpose usability.\nEvaluation scripts will be released at https://github.com/jylei16/Imagine-e.", "link": "http://arxiv.org/abs/2501.13920v1", "categories": ["cs.CV", "cs.CL", "cs.LG"]}, {"title": "Temporal Preference Optimization for Long-Form Video Understanding", "authors": ["Rui Li", "Xiaohan Wang", "Yuhui Zhang", "Zeyu Wang", "Serena Yeung-Levy"], "published": "2025-01-23T18:58:03Z", "summary": "Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps://ruili33.github.io/tpo_website.", "link": "http://arxiv.org/abs/2501.13919v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"]}, {"title": "Improving Video Generation with Human Feedback", "authors": ["Jie Liu", "Gongye Liu", "Jiajun Liang", "Ziyang Yuan", "Xiaokun Liu", "Mingwu Zheng", "Xiele Wu", "Qiulin Wang", "Wenyu Qin", "Menghan Xia", "Xintao Wang", "Xiaohong Liu", "Fei Yang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Yujiu Yang", "Wanli Ouyang"], "published": "2025-01-23T18:55:41Z", "summary": "Video generation has achieved significant advances through rectified flow\ntechniques, but issues like unsmooth motion and misalignment between videos and\nprompts persist. In this work, we develop a systematic pipeline that harnesses\nhuman feedback to mitigate these problems and refine the video generation\nmodel. Specifically, we begin by constructing a large-scale human preference\ndataset focused on modern video generation models, incorporating pairwise\nannotations across multi-dimensions. We then introduce VideoReward, a\nmulti-dimensional video reward model, and examine how annotations and various\ndesign choices impact its rewarding efficacy. From a unified reinforcement\nlearning perspective aimed at maximizing reward with KL regularization, we\nintroduce three alignment algorithms for flow-based models by extending those\nfrom diffusion models. These include two training-time strategies: direct\npreference optimization for flow (Flow-DPO) and reward weighted regression for\nflow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies\nreward guidance directly to noisy videos. Experimental results indicate that\nVideoReward significantly outperforms existing reward models, and Flow-DPO\ndemonstrates superior performance compared to both Flow-RWR and standard\nsupervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom\nweights to multiple objectives during inference, meeting personalized video\nquality needs. Project page: https://gongyeliu.github.io/videoalign.", "link": "http://arxiv.org/abs/2501.13918v1", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"]}, {"title": "PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy", "authors": ["Linh Tran", "Timothy Castiglia", "Stacy Patterson", "Ana Milanova"], "published": "2025-01-23T18:53:43Z", "summary": "We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),\na communication-efficient Vertical Federated Learning algorithm with\nDifferential Privacy guarantees. PBM-VFL combines Secure Multi-Party\nComputation with the recently introduced Poisson Binomial Mechanism to protect\nparties' private datasets during model training. We define the novel concept of\nfeature privacy and analyze end-to-end feature and sample privacy of our\nalgorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We\nalso provide the first theoretical characterization of the relationship between\nprivacy budget, convergence error, and communication cost in\ndifferentially-private VFL. Finally, we empirically show that our model\nperforms well with high levels of privacy.", "link": "http://arxiv.org/abs/2501.13916v1", "categories": ["cs.LG"]}, {"title": "Binary Diffusion Probabilistic Model", "authors": ["Vitaliy Kinakh", "Slava Voloshynovskiy"], "published": "2025-01-23T18:52:47Z", "summary": "We introduce the Binary Diffusion Probabilistic Model (BDPM), a novel\ngenerative model optimized for binary data representations. While denoising\ndiffusion probabilistic models (DDPMs) have demonstrated notable success in\ntasks like image synthesis and restoration, traditional DDPMs rely on\ncontinuous data representations and mean squared error (MSE) loss for training,\napplying Gaussian noise models that may not be optimal for discrete or binary\ndata structures. BDPM addresses this by decomposing images into bitplanes and\nemploying XOR-based noise transformations, with a denoising model trained using\nbinary cross-entropy loss. This approach enables precise noise control and\ncomputationally efficient inference, significantly lowering computational costs\nand improving model convergence. When evaluated on image restoration tasks such\nas image super-resolution, inpainting, and blind image restoration, BDPM\noutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQ\ndatasets. Notably, BDPM requires fewer inference steps than traditional DDPM\nmodels to reach optimal results, showcasing enhanced inference efficiency.", "link": "http://arxiv.org/abs/2501.13915v1", "categories": ["cs.CV"]}, {"title": "Analysis of Indic Language Capabilities in LLMs", "authors": ["Aatman Vaidya", "Tarunima Prabhakar", "Denny George", "Swair Shah"], "published": "2025-01-23T18:49:33Z", "summary": "This report evaluates the performance of text-in text-out Large Language\nModels (LLMs) to understand and generate Indic languages. This evaluation is\nused to identify and prioritize Indic languages suited for inclusion in safety\nbenchmarks. We conduct this study by reviewing existing evaluation studies and\ndatasets; and a set of twenty-eight LLMs that support Indic languages. We\nanalyze the LLMs on the basis of the training data, license for model and data,\ntype of access and model developers. We also compare Indic language performance\nacross evaluation datasets and find that significant performance disparities in\nperformance across Indic languages. Hindi is the most widely represented\nlanguage in models. While model performance roughly correlates with number of\nspeakers for the top five languages, the assessment after that varies.", "link": "http://arxiv.org/abs/2501.13912v1", "categories": ["cs.CL"]}, {"title": "On Learning Representations for Tabular Data Distillation", "authors": ["Inwon Kang", "Parikshit Ram", "Yi Zhou", "Horst Samulowitz", "Oshani Seneviratne"], "published": "2025-01-23T18:35:15Z", "summary": "Dataset distillation generates a small set of information-rich instances from\na large dataset, resulting in reduced storage requirements, privacy or\ncopyright risks, and computational costs for downstream modeling, though much\nof the research has focused on the image data modality. We study tabular data\ndistillation, which brings in novel challenges such as the inherent feature\nheterogeneity and the common use of non-differentiable learning models (such as\ndecision tree ensembles and nearest-neighbor predictors). To mitigate these\nchallenges, we present $\\texttt{TDColER}$, a tabular data distillation\nframework via column embeddings-based representation learning. To evaluate this\nframework, we also present a tabular data distillation benchmark, ${{\\sf \\small\nTDBench}}$. Based on an elaborate evaluation on ${{\\sf \\small TDBench}}$,\nresulting in 226,890 distilled datasets and 548,880 models trained on them, we\ndemonstrate that $\\texttt{TDColER}$ is able to boost the distilled data quality\nof off-the-shelf distillation schemes by 0.5-143% across 7 different tabular\nlearning models.", "link": "http://arxiv.org/abs/2501.13905v1", "categories": ["cs.LG"]}, {"title": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models", "authors": ["Linh Tran", "Wei Sun", "Stacy Patterson", "Ana Milanova"], "published": "2025-01-23T18:34:09Z", "summary": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank adaptation scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.", "link": "http://arxiv.org/abs/2501.13904v1", "categories": ["cs.LG"]}, {"title": "PointOBB-v3: Expanding Performance Boundaries of Single Point-Supervised\n  Oriented Object Detection", "authors": ["Peiyuan Zhang", "Junwei Luo", "Xue Yang", "Yi Yu", "Qingyun Li", "Yue Zhou", "Xiaosong Jia", "Xudong Lu", "Jingdong Chen", "Xiang Li", "Junchi Yan", "Yansheng Li"], "published": "2025-01-23T18:18:15Z", "summary": "With the growing demand for oriented object detection (OOD), recent studies\non point-supervised OOD have attracted significant interest. In this paper, we\npropose PointOBB-v3, a stronger single point-supervised OOD framework. Compared\nto existing methods, it generates pseudo rotated boxes without additional\npriors and incorporates support for the end-to-end paradigm. PointOBB-v3\nfunctions by integrating three unique image views: the original view, a resized\nview, and a rotated/flipped (rot/flp) view. Based on the views, a scale\naugmentation module and an angle acquisition module are constructed. In the\nfirst module, a Scale-Sensitive Consistency (SSC) loss and a Scale-Sensitive\nFeature Fusion (SSFF) module are introduced to improve the model's ability to\nestimate object scale. To achieve precise angle predictions, the second module\nemploys symmetry-based self-supervised learning. Additionally, we introduce an\nend-to-end version that eliminates the pseudo-label generation process by\nintegrating a detector branch and introduces an Instance-Aware Weighting (IAW)\nstrategy to focus on high-quality predictions. We conducted extensive\nexperiments on the DIOR-R, DOTA-v1.0/v1.5/v2.0, FAIR1M, STAR, and RSAR\ndatasets. Across all these datasets, our method achieves an average improvement\nin accuracy of 3.56% in comparison to previous state-of-the-art methods. The\ncode will be available at https://github.com/ZpyWHU/PointOBB-v3.", "link": "http://arxiv.org/abs/2501.13898v1", "categories": ["cs.CV", "cs.AI"]}, {"title": "GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous\n  Exploration", "authors": ["Yue Fan", "Handong Zhao", "Ruiyi Zhang", "Yu Shen", "Xin Eric Wang", "Gang Wu"], "published": "2025-01-23T18:16:21Z", "summary": "Graphical User Interface (GUI) action grounding is a critical step in GUI\nautomation that maps language instructions to actionable elements on GUI\nscreens. Most recent works of GUI action grounding leverage large GUI datasets\nto fine-tune MLLMs. However, the fine-tuning data always covers limited GUI\nenvironments, and we find the performance of the resulting model deteriorates\nin novel environments. We argue that the GUI grounding models should be further\naligned to the novel environments to reveal their full potential, when the\ninference is known to involve novel environments, i.e., environments not used\nduring the previous fine-tuning. To realize this, we first propose GUI-Bee, an\nMLLM-based autonomous agent, to collect high-quality, environment-specific data\nthrough exploration and then continuously fine-tune GUI grounding models with\nthe collected data. Our agent leverages a novel Q-value-Incentive In-Context\nReinforcement Learning (Q-ICRL) method to optimize exploration efficiency and\ndata quality. Additionally, we introduce NovelScreenSpot, a benchmark for\ntesting how well the data can help align GUI action grounding models to novel\nenvironments and demonstrate the effectiveness of data collected by GUI-Bee in\nthe experiments. Furthermore, we conduct an ablation study to validate the\nQ-ICRL method in enhancing the efficiency of GUI-Bee. Project page:\nhttps://gui-bee.github.io", "link": "http://arxiv.org/abs/2501.13896v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"]}, {"title": "Pix2Cap-COCO: Advancing Visual Comprehension via Pixel-Level Captioning", "authors": ["Zuyao You", "Junke Wang", "Lingyu Kong", "Bo He", "Zuxuan Wu"], "published": "2025-01-23T18:08:57Z", "summary": "We present Pix2Cap-COCO, the first panoptic pixel-level caption dataset\ndesigned to advance fine-grained visual understanding. To achieve this, we\ncarefully design an automated annotation pipeline that prompts GPT-4V to\ngenerate pixel-aligned, instance-specific captions for individual objects\nwithin images, enabling models to learn more granular relationships between\nobjects and their contexts. This approach results in 167,254 detailed captions,\nwith an average of 22.94 words per caption. Building on Pix2Cap-COCO, we\nintroduce a novel task, panoptic segmentation-captioning, which challenges\nmodels to recognize instances in an image and provide detailed descriptions for\neach simultaneously. To benchmark this task, we design a robust baseline based\non X-Decoder. The experimental results demonstrate that Pix2Cap-COCO is a\nparticularly challenging dataset, as it requires models to excel in both\nfine-grained visual understanding and detailed language generation.\nFurthermore, we leverage Pix2Cap-COCO for Supervised Fine-Tuning (SFT) on large\nmultimodal models (LMMs) to enhance their performance. For example, training\nwith Pix2Cap-COCO significantly improves the performance of GPT4RoI, yielding\ngains in CIDEr +1.4%, ROUGE +0.4%, and SPICE +0.5% on Visual Genome dataset,\nand strengthens its region understanding ability on the ViP-BENCH, with an\noverall improvement of +5.1%, including notable increases in recognition\naccuracy +11.2% and language generation quality +22.2%.", "link": "http://arxiv.org/abs/2501.13893v1", "categories": ["cs.CV", "cs.AI", "cs.LG"]}, {"title": "Federated Granger Causality Learning for Interdependent Clients with\n  State Space Representation", "authors": ["Ayush Mohanty", "Nazal Mohamed", "Paritosh Ramanan", "Nagi Gebraeel"], "published": "2025-01-23T18:04:21Z", "summary": "Advanced sensors and IoT devices have improved the monitoring and control of\ncomplex industrial enterprises. They have also created an interdependent fabric\nof geographically distributed process operations (clients) across these\nenterprises. Granger causality is an effective approach to detect and quantify\ninterdependencies by examining how one client's state affects others over time.\nUnderstanding these interdependencies captures how localized events, such as\nfaults and disruptions, can propagate throughout the system, possibly causing\nwidespread operational impacts. However, the large volume and complexity of\nindustrial data pose challenges in modeling these interdependencies. This paper\ndevelops a federated approach to learning Granger causality. We utilize a\nlinear state space system framework that leverages low-dimensional state\nestimates to analyze interdependencies. This addresses bandwidth limitations\nand the computational burden commonly associated with centralized data\nprocessing. We propose augmenting the client models with the Granger causality\ninformation learned by the server through a Machine Learning (ML) function. We\nexamine the co-dependence between the augmented client and server models and\nreformulate the framework as a standalone ML algorithm providing conditions for\nits sublinear and linear convergence rates. We also study the convergence of\nthe framework to a centralized oracle model. Moreover, we include a\ndifferential privacy analysis to ensure data security while preserving causal\ninsights. Using synthetic data, we conduct comprehensive experiments to\ndemonstrate the robustness of our approach to perturbations in causality, the\nscalability to the size of communication, number of clients, and the dimensions\nof raw data. We also evaluate the performance on two real-world industrial\ncontrol system datasets by reporting the volume of data saved by\ndecentralization.", "link": "http://arxiv.org/abs/2501.13890v1", "categories": ["cs.LG", "stat.ML"]}, {"title": "Generating Realistic Forehead-Creases for User Verification via\n  Conditioned Piecewise Polynomial Curves", "authors": ["Abhishek Tandon", "Geetanjali Sharma", "Gaurav Jaswal", "Aditya Nigam", "Raghavendra Ramachandra"], "published": "2025-01-23T18:01:19Z", "summary": "We propose a trait-specific image generation method that models forehead\ncreases geometrically using B-spline and B\\'ezier curves. This approach ensures\nthe realistic generation of both principal creases and non-prominent crease\npatterns, effectively constructing detailed and authentic forehead-crease\nimages. These geometrically rendered images serve as visual prompts for a\ndiffusion-based Edge-to-Image translation model, which generates corresponding\nmated samples. The resulting novel synthetic identities are then used to train\na forehead-crease verification network. To enhance intra-subject diversity in\nthe generated samples, we employ two strategies: (a) perturbing the control\npoints of B-splines under defined constraints to maintain label consistency,\nand (b) applying image-level augmentations to the geometric visual prompts,\nsuch as dropout and elastic transformations, specifically tailored to crease\npatterns. By integrating the proposed synthetic dataset with real-world data,\nour method significantly improves the performance of forehead-crease\nverification systems under a cross-database verification protocol.", "link": "http://arxiv.org/abs/2501.13889v1", "categories": ["cs.CV"]}, {"title": "Multimodal Sensor Dataset for Monitoring Older Adults Post Lower-Limb\n  Fractures in Community Settings", "authors": ["Ali Abedi", "Charlene H. Chu", "Shehroz S. Khan"], "published": "2025-01-23T18:01:01Z", "summary": "Lower-Limb Fractures (LLF) are a major health concern for older adults, often\nleading to reduced mobility and prolonged recovery, potentially impairing daily\nactivities and independence. During recovery, older adults frequently face\nsocial isolation and functional decline, complicating rehabilitation and\nadversely affecting physical and mental health. Multi-modal sensor platforms\nthat continuously collect data and analyze it using machine-learning algorithms\ncan remotely monitor this population and infer health outcomes. They can also\nalert clinicians to individuals at risk of isolation and decline. This paper\npresents a new publicly available multi-modal sensor dataset, MAISON-LLF,\ncollected from older adults recovering from LLF in community settings. The\ndataset includes data from smartphone and smartwatch sensors, motion detectors,\nsleep-tracking mattresses, and clinical questionnaires on isolation and\ndecline. The dataset was collected from ten older adults living alone at home\nfor eight weeks each, totaling 560 days of 24-hour sensor data. For technical\nvalidation, supervised machine-learning and deep-learning models were developed\nusing the sensor and clinical questionnaire data, providing a foundational\ncomparison for the research community.", "link": "http://arxiv.org/abs/2501.13888v1", "categories": ["cs.LG", "cs.CV"]}, {"title": "What Does an Audio Deepfake Detector Focus on? A Study in the Time\n  Domain", "authors": ["Petr Grinberg", "Ankur Kumar", "Surya Koppisetti", "Gaurav Bharaj"], "published": "2025-01-23T18:00:14Z", "summary": "Adding explanations to audio deepfake detection (ADD) models will boost their\nreal-world application by providing insight on the decision making process. In\nthis paper, we propose a relevancy-based explainable AI (XAI) method to analyze\nthe predictions of transformer-based ADD models. We compare against standard\nGrad-CAM and SHAP-based methods, using quantitative faithfulness metrics as\nwell as a partial spoof test, to comprehensively analyze the relative\nimportance of different temporal regions in an audio. We consider large\ndatasets, unlike previous works where only limited utterances are studied, and\nfind that the XAI methods differ in their explanations. The proposed\nrelevancy-based XAI method performs the best overall on a variety of metrics.\nFurther investigation on the relative importance of speech/non-speech, phonetic\ncontent, and voice onsets/offsets suggest that the XAI results obtained from\nanalyzing limited utterances don't necessarily hold when evaluated on large\ndatasets.", "link": "http://arxiv.org/abs/2501.13887v1", "categories": ["cs.LG", "cs.SD", "eess.AS"]}, {"title": "Exploring Finetuned Audio-LLM on Heart Murmur Features", "authors": ["Adrian Florea", "Xilin Jiang", "Nima Mesgarani", "Xiaofan Jiang"], "published": "2025-01-23T17:57:18Z", "summary": "Large language models (LLMs) for audio have excelled in recognizing and\nanalyzing human speech, music, and environmental sounds. However, their\npotential for understanding other types of sounds, particularly biomedical\nsounds, remains largely underexplored despite significant scientific interest.\nIn this study, we focus on diagnosing cardiovascular diseases using\nphonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN)\nparadigms are restricted to heart murmur classification (healthy vs unhealthy)\nand do not predict other acoustic features of the murmur such as timing,\ngrading, harshness, pitch, and quality, which are important in helping\nphysicians diagnose the underlying heart conditions. We propose to finetune an\naudio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG)\ndataset and evaluate its performance in classifying 11 expert-labeled murmur\nfeatures. Additionally, we aim to achieve more noise-robust and generalizable\nsystem by exploring a preprocessing segmentation algorithm using an audio\nrepresentation model, SSAMBA. Our results indicate that the LLM-based model\noutperforms state-of-the-art methods in 8 of the 11 features and performs\ncomparably in the remaining 3. Moreover, the LLM successfully classifies\nlong-tail murmur features with limited training data, a task that all previous\nmethods have failed to classify. These findings underscore the potential of\naudio LLMs as assistants to human cardiologists in enhancing heart disease\ndiagnosis.", "link": "http://arxiv.org/abs/2501.13884v1", "categories": ["eess.AS", "cs.AI", "cs.SD"]}, {"title": "Utilizing Evolution Strategies to Train Transformers in Reinforcement\n  Learning", "authors": ["Maty\u00e1\u0161 Lorenc"], "published": "2025-01-23T17:56:40Z", "summary": "We explore a capability of evolution strategies to train an agent with its\npolicy based on a transformer architecture in a reinforcement learning setting.\nWe performed experiments using OpenAI's highly parallelizable evolution\nstrategy to train Decision Transformer in Humanoid locomotion environment and\nin the environment of Atari games, testing the ability of this black-box\noptimization technique to train even such relatively large and complicated\nmodels (compared to those previously tested in the literature). We also\nproposed a method to aid the training by first pretraining the model before\nusing the OpenAI-ES to train it further, and tested its effectiveness. The\nexamined evolution strategy proved to be, in general, capable of achieving\nstrong results and managed to obtain high-performing agents. Therefore, the\npretraining was shown to be unnecessary; yet still, it helped us observe and\nformulate several further insights.", "link": "http://arxiv.org/abs/2501.13883v1", "categories": ["cs.LG", "cs.NE"]}, {"title": "A RAG-Based Institutional Assistant", "authors": ["Gustavo Kuratomi", "Paulo Pirozelli", "Fabio G. Cozman", "Sarajane M. Peres"], "published": "2025-01-23T17:54:19Z", "summary": "Although large language models (LLMs) demonstrate strong text generation\ncapabilities, they struggle in scenarios requiring access to structured\nknowledge bases or specific documents, limiting their effectiveness in\nknowledge-intensive tasks. To address this limitation, retrieval-augmented\ngeneration (RAG) models have been developed, enabling generative models to\nincorporate relevant document fragments into their inputs. In this paper, we\ndesign and evaluate a RAG-based virtual assistant specifically tailored for the\nUniversity of S\\~ao Paulo. Our system architecture comprises two key modules: a\nretriever and a generative model. We experiment with different types of models\nfor both components, adjusting hyperparameters such as chunk size and the\nnumber of retrieved documents. Our optimal retriever model achieves a Top-5\naccuracy of 30%, while our most effective generative model scores 22.04\\%\nagainst ground truth answers. Notably, when the correct document chunks are\nsupplied to the LLMs, accuracy significantly improves to 54.02%, an increase of\nover 30 percentage points. Conversely, without contextual input, performance\ndeclines to 13.68%. These findings highlight the critical role of database\naccess in enhancing LLM performance. They also reveal the limitations of\ncurrent semantic search methods in accurately identifying relevant documents\nand underscore the ongoing challenges LLMs face in generating precise\nresponses.", "link": "http://arxiv.org/abs/2501.13880v1", "categories": ["cs.CL"]}, {"title": "Eye Gaze as a Signal for Conveying User Attention in Contextual AI\n  Systems", "authors": ["Ethan Wilson", "Naveen Sendhilnathan", "Charlie S. Burlingham", "Yusuf Mansour", "Robert Cavin", "Sai Deep Tetali", "Ajoy Savio Fernandes", "Michael J. Proulx"], "published": "2025-01-23T17:51:54Z", "summary": "Advanced multimodal AI agents can now collaborate with users to solve\nchallenges in the world. We explore eye tracking's role in such interaction to\nconvey a user's attention relative to the physical environment. We hypothesize\nthat this knowledge improves contextual understanding for AI agents. By\nobserving hours of human-object interactions, we first measure the relationship\nbetween an eye tracker's signal quality and its ability to reliably place gaze\non nearby physical objects. We then conduct experiments which relay the user's\nscanpath history as additional context querying multimodal agents. Our results\nshow that eye tracking provides high value as a user attention signal and can\nconvey information about the user's current task and interests to the agent.", "link": "http://arxiv.org/abs/2501.13878v1", "categories": ["cs.HC", "cs.CV"]}, {"title": "FAST-LIVO2 on Resource-Constrained Platforms: LiDAR-Inertial-Visual\n  Odometry with Efficient Memory and Computation", "authors": ["Bingyang Zhou", "Chunran Zheng", "Ziming Wang", "Fangcheng Zhu", "Yixi Cai", "Fu Zhang"], "published": "2025-01-23T17:49:49Z", "summary": "This paper presents a lightweight LiDAR-inertial-visual odometry system\noptimized for resource-constrained platforms. It integrates a\ndegeneration-aware adaptive visual frame selector into error-state iterated\nKalman filter (ESIKF) with sequential updates, improving computation efficiency\nsignificantly while maintaining a similar level of robustness. Additionally, a\nmemory-efficient mapping structure combining a locally unified visual-LiDAR map\nand a long-term visual map achieves a good trade-off between performance and\nmemory usage. Extensive experiments on x86 and ARM platforms demonstrate the\nsystem's robustness and efficiency. On the Hilti dataset, our system achieves a\n33% reduction in per-frame runtime and 47% lower memory usage compared to\nFAST-LIVO2, with only a 3 cm increase in RMSE. Despite this slight accuracy\ntrade-off, our system remains competitive, outperforming state-of-the-art\n(SOTA) LIO methods such as FAST-LIO2 and most existing LIVO systems. These\nresults validate the system's capability for scalable deployment on\nresource-constrained edge computing platforms.", "link": "http://arxiv.org/abs/2501.13876v1", "categories": ["cs.RO"]}, {"title": "Autoencoders for Anomaly Detection are Unreliable", "authors": ["Roel Bouman", "Tom Heskes"], "published": "2025-01-23T17:36:48Z", "summary": "Autoencoders are frequently used for anomaly detection, both in the\nunsupervised and semi-supervised settings. They rely on the assumption that\nwhen trained using the reconstruction loss, they will be able to reconstruct\nnormal data more accurately than anomalous data. Some recent works have posited\nthat this assumption may not always hold, but little has been done to study the\nvalidity of the assumption in theory. In this work we show that this assumption\nindeed does not hold, and illustrate that anomalies, lying far away from normal\ndata, can be perfectly reconstructed in practice. We revisit the theory of\nfailure of linear autoencoders for anomaly detection by showing how they can\nperfectly reconstruct out of bounds, or extrapolate undesirably, and note how\nthis can be dangerous in safety critical applications. We connect this to\nnon-linear autoencoders through experiments on both tabular data and real-world\nimage data, the two primary application areas of autoencoders for anomaly\ndetection.", "link": "http://arxiv.org/abs/2501.13864v1", "categories": ["cs.LG", "cs.AI"]}, {"title": "Dual-Modal Prototype Joint Learning for Compositional Zero-Shot Learning", "authors": ["Shiyu Zhang", "Cheng Yan", "Yang Liu", "Chenchen Jing", "Lei Zhou", "Wenjun Wang"], "published": "2025-01-23T17:30:27Z", "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions\nof attributes and objects by leveraging knowledge learned from seen\ncompositions. Recent approaches have explored the use of Vision-Language Models\n(VLMs) to align textual and visual modalities. These methods typically employ\nprompt engineering, parameter-tuning, and modality fusion to generate rich\ntextual prototypes that serve as class prototypes for CZSL. However, the\nmodality gap results in textual prototypes being unable to fully capture the\noptimal representations of all class prototypes, particularly those with\nfine-grained features, which can be directly obtained from the visual modality.\nIn this paper, we propose a novel Dual-Modal Prototype Joint Learning framework\nfor the CZSL task. Our approach, based on VLMs, introduces prototypes in both\nthe textual and visual modalities. The textual prototype is optimized to\ncapture broad conceptual information, aiding the model's generalization across\nunseen compositions. Meanwhile, the visual prototype is used to mitigate the\nclassification errors caused by the modality gap and capture fine-grained\ndetails to distinguish images with similar appearances. To effectively optimize\nthese prototypes, we design specialized decomposition modules and a joint\nlearning strategy that enrich the features from both modalities. These\nprototypes not only capture key category information during training but also\nserve as crucial reference targets during inference. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance in the\nclosed-world setting and competitive performance in the open-world setting\nacross three publicly available CZSL benchmarks. These findings validate the\neffectiveness of our method in advancing compositional generalization.", "link": "http://arxiv.org/abs/2501.13859v1", "categories": ["cs.CV"]}, {"title": "First Lessons Learned of an Artificial Intelligence Robotic System for\n  Autonomous Coarse Waste Recycling Using Multispectral Imaging-Based Methods", "authors": ["Timo Lange", "Ajish Babu", "Philipp Meyer", "Matthis Keppner", "Tim Tiedemann", "Martin Wittmaier", "Sebastian Wolff", "Thomas V\u00f6gele"], "published": "2025-01-23T17:24:24Z", "summary": "Current disposal facilities for coarse-grained waste perform manual sorting\nof materials with heavy machinery. Large quantities of recyclable materials are\nlost to coarse waste, so more effective sorting processes must be developed to\nrecover them. Two key aspects to automate the sorting process are object\ndetection with material classification in mixed piles of waste, and autonomous\ncontrol of hydraulic machinery. Because most objects in those accumulations of\nwaste are damaged or destroyed, object detection alone is not feasible in the\nmajority of cases. To address these challenges, we propose a classification of\nmaterials with multispectral images of ultraviolet (UV), visual (VIS), near\ninfrared (NIR), and short-wave infrared (SWIR) spectrums. Solution for\nautonomous control of hydraulic heavy machines for sorting of bulky waste is\nbeing investigated using cost-effective cameras and artificial\nintelligence-based controllers.", "link": "http://arxiv.org/abs/2501.13855v1", "categories": ["cs.CV", "cs.LG", "cs.RO"]}, {"title": "Large Vision-Language Models for Knowledge-Grounded Data Annotation of\n  Memes", "authors": ["Shiling Deng", "Serge Belongie", "Peter Ebert Christensen"], "published": "2025-01-23T17:18:30Z", "summary": "Memes have emerged as a powerful form of communication, integrating visual\nand textual elements to convey humor, satire, and cultural messages. Existing\nresearch has focused primarily on aspects such as emotion classification, meme\ngeneration, propagation, interpretation, figurative language, and\nsociolinguistics, but has often overlooked deeper meme comprehension and\nmeme-text retrieval. To address these gaps, this study introduces\nClassicMemes-50-templates (CM50), a large-scale dataset consisting of over\n33,000 memes, centered around 50 popular meme templates. We also present an\nautomated knowledge-grounded annotation pipeline leveraging large\nvision-language models to produce high-quality image captions, meme captions,\nand literary device labels overcoming the labor intensive demands of manual\nannotation. Additionally, we propose a meme-text retrieval CLIP model (mtrCLIP)\nthat utilizes cross-modal embedding to enhance meme analysis, significantly\nimproving retrieval performance. Our contributions include:(1) a novel dataset\nfor large-scale meme study, (2) a scalable meme annotation framework, and (3) a\nfine-tuned CLIP for meme-text retrieval, all aimed at advancing the\nunderstanding and analysis of memes at scale.", "link": "http://arxiv.org/abs/2501.13851v1", "categories": ["cs.LG"]}, {"title": "Where Do You Go? Pedestrian Trajectory Prediction using Scene Features", "authors": ["Mohammad Ali Rezaei", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "published": "2025-01-23T17:15:26Z", "summary": "Accurate prediction of pedestrian trajectories is crucial for enhancing the\nsafety of autonomous vehicles and reducing traffic fatalities involving\npedestrians. While numerous studies have focused on modeling interactions among\npedestrians to forecast their movements, the influence of environmental factors\nand scene-object placements has been comparatively underexplored. In this\npaper, we present a novel trajectory prediction model that integrates both\npedestrian interactions and environmental context to improve prediction\naccuracy. Our approach captures spatial and temporal interactions among\npedestrians within a sparse graph framework. To account for pedestrian-scene\ninteractions, we employ advanced image enhancement and semantic segmentation\ntechniques to extract detailed scene features. These scene and interaction\nfeatures are then fused through a cross-attention mechanism, enabling the model\nto prioritize relevant environmental factors that influence pedestrian\nmovements. Finally, a temporal convolutional network processes the fused\nfeatures to predict future pedestrian trajectories. Experimental results\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches, achieving ADE and FDE values of 0.252 and 0.372 meters,\nrespectively, underscoring the importance of incorporating both social\ninteractions and environmental context in pedestrian trajectory prediction.", "link": "http://arxiv.org/abs/2501.13848v1", "categories": ["cs.CV", "cs.AI", "cs.LG"]}, {"title": "Think Outside the Data: Colonial Biases and Systemic Issues in Automated\n  Moderation Pipelines for Low-Resource Languages", "authors": ["Farhana Shahid", "Mona Elswah", "Aditya Vashistha"], "published": "2025-01-23T17:01:53Z", "summary": "Most social media users come from non-English speaking countries in the\nGlobal South. Despite the widespread prevalence of harmful content in these\nregions, current moderation systems repeatedly struggle in low-resource\nlanguages spoken there. In this work, we examine the challenges AI researchers\nand practitioners face when building moderation tools for low-resource\nlanguages. We conducted semi-structured interviews with 22 AI researchers and\npractitioners specializing in automatic detection of harmful content in four\ndiverse low-resource languages from the Global South. These are: Tamil from\nSouth Asia, Swahili from East Africa, Maghrebi Arabic from North Africa, and\nQuechua from South America. Our findings reveal that social media companies'\nrestrictions on researchers' access to data exacerbate the historical\nmarginalization of these languages, which have long lacked datasets for\nstudying online harms. Moreover, common preprocessing techniques and language\nmodels, predominantly designed for data-rich English, fail to account for the\nlinguistic complexity of low-resource languages. This leads to critical errors\nwhen moderating content in Tamil, Swahili, Arabic, and Quechua, which are\nmorphologically richer than English. Based on our findings, we establish that\nthe precarities in current moderation pipelines are rooted in deep systemic\ninequities and continue to reinforce historical power imbalances. We conclude\nby discussing multi-stakeholder approaches to improve moderation for\nlow-resource languages.", "link": "http://arxiv.org/abs/2501.13836v1", "categories": ["cs.CL", "cs.HC"]}, {"title": "On the Reasoning Capacity of AI Models and How to Quantify It", "authors": ["Santosh Kumar Radha", "Oktay Goktas"], "published": "2025-01-23T16:58:18Z", "summary": "Recent advances in Large Language Models (LLMs) have intensified the debate\nsurrounding the fundamental nature of their reasoning capabilities. While\nachieving high performance on benchmarks such as GPQA and MMLU, these models\nexhibit limitations in more complex reasoning tasks, highlighting the need for\nmore rigorous evaluation methodologies. We propose a novel phenomenological\napproach that goes beyond traditional accuracy metrics to probe the underlying\nmechanisms of model behavior, establishing a framework that could broadly\nimpact how we analyze and understand AI systems. Using positional bias in\nmultiple-choice reasoning tasks as a case study, we demonstrate how systematic\nperturbations can reveal fundamental aspects of model decision-making. To\nanalyze these behaviors, we develop two complementary phenomenological models:\na Probabilistic Mixture Model (PMM) that decomposes model responses into\nreasoning, memorization, and guessing components and an Information-Theoretic\nConsistency (ITC) analysis that quantifies the relationship between model\nconfidence and strategy selection. Through controlled experiments on reasoning\nbenchmarks, we show that true reasoning remains challenging for current models,\nwith apparent success often relying on sophisticated combinations of\nmemorization and pattern matching rather than genuine logical deduction. More\nfundamentally, we demonstrate that accuracy alone often overstates a model's\nreasoning abilities, as model behavior can be characterized through underlying\nmechanisms in the phase space of cognitive strategies, revealing how models\ndynamically balance different approaches when responding to queries. This\nframework enables quantitative criteria for real-world deployments, allowing\napplications to specify reliability thresholds based on strategy distributions\nrather than aggregate performance metrics.", "link": "http://arxiv.org/abs/2501.13833v1", "categories": ["cs.AI", "cs.CL", "cs.IT", "math.IT"]}, {"title": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR\n  Post Editing", "authors": ["Hao Zhang", "Felix Stahlberg", "Shankar Kumar"], "published": "2025-01-23T16:54:27Z", "summary": "Large Language Models (LLMs) excel at rewriting tasks such as text style\ntransfer and grammatical error correction. While there is considerable overlap\nbetween the inputs and outputs in these tasks, the decoding cost still\nincreases with output length, regardless of the amount of overlap. By\nleveraging the overlap between the input and the output, Kaneko and Okazaki\n(2023) proposed model-agnostic edit span representations to compress the\nrewrites to save computation. They reported an output length reduction rate of\nnearly 80% with minimal accuracy impact in four rewriting tasks. In this paper,\nwe propose alternative edit phrase representations inspired by phrase-based\nstatistical machine translation. We systematically compare our phrasal\nrepresentations with their span representations. We apply the LLM rewriting\nmodel to the task of Automatic Speech Recognition (ASR) post editing and show\nthat our target-phrase-only edit representation has the best\nefficiency-accuracy trade-off. On the LibriSpeech test set, our method closes\n50-60% of the WER gap between the edit span model and the full rewrite model\nwhile losing only 10-20% of the length reduction rate of the edit span model.", "link": "http://arxiv.org/abs/2501.13831v1", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, {"title": "A space-decoupling framework for optimization on bounded-rank matrices\n  with orthogonally invariant constraints", "authors": ["Yan Yang", "Bin Gao", "Ya-xiang Yuan"], "published": "2025-01-23T16:54:03Z", "summary": "Imposing additional constraints on low-rank optimization has garnered growing\ninterest. However, the geometry of coupled constraints hampers the\nwell-developed low-rank structure and makes the problem intricate. To this end,\nwe propose a space-decoupling framework for optimization on bounded-rank\nmatrices with orthogonally invariant constraints. The ``space-decoupling\" is\nreflected in several ways. We show that the tangent cone of coupled constraints\nis the intersection of tangent cones of each constraint. Moreover, we decouple\nthe intertwined bounded-rank and orthogonally invariant constraints into two\nspaces, leading to optimization on a smooth manifold. Implementing Riemannian\nalgorithms on this manifold is painless as long as the geometry of additional\nconstraints is known. In addition, we unveil the equivalence between the\nreformulated problem and the original problem. Numerical experiments on\nreal-world applications -- spherical data fitting, graph similarity measuring,\nlow-rank SDP, model reduction of Markov processes, reinforcement learning, and\ndeep learning -- validate the superiority of the proposed framework.", "link": "http://arxiv.org/abs/2501.13830v1", "categories": ["math.OC", "cs.AI", "cs.LG"]}, {"title": "MV-GMN: State Space Model for Multi-View Action Recognition", "authors": ["Yuhui Lin", "Jiaxuan Lu", "Yue Yong", "Jiahao Zhang"], "published": "2025-01-23T16:53:46Z", "summary": "Recent advancements in multi-view action recognition have largely relied on\nTransformer-based models. While effective and adaptable, these models often\nrequire substantial computational resources, especially in scenarios with\nmultiple views and multiple temporal sequences. Addressing this limitation,\nthis paper introduces the MV-GMN model, a state-space model specifically\ndesigned to efficiently aggregate multi-modal data (RGB and skeleton),\nmulti-view perspectives, and multi-temporal information for action recognition\nwith reduced computational complexity. The MV-GMN model employs an innovative\nMulti-View Graph Mamba network comprising a series of MV-GMN blocks. Each block\nincludes a proposed Bidirectional State Space Block and a GCN module. The\nBidirectional State Space Block introduces four scanning strategies, including\nview-prioritized and time-prioritized approaches. The GCN module leverages\nrule-based and KNN-based methods to construct the graph network, effectively\nintegrating features from different viewpoints and temporal instances.\nDemonstrating its efficacy, MV-GMN outperforms the state-of-the-arts on several\ndatasets, achieving notable accuracies of 97.3\\% and 96.7\\% on the NTU RGB+D\n120 dataset in cross-subject and cross-view scenarios, respectively. MV-GMN\nalso surpasses Transformer-based baselines while requiring only linear\ninference complexity, underscoring the model's ability to reduce computational\nload and enhance the scalability and applicability of multi-view action\nrecognition technologies.", "link": "http://arxiv.org/abs/2501.13829v1", "categories": ["cs.CV"]}, {"title": "PhotoGAN: Generative Adversarial Neural Network Acceleration with\n  Silicon Photonics", "authors": ["Tharini Suresh", "Salma Afifi", "Sudeep Pasricha"], "published": "2025-01-23T16:53:31Z", "summary": "Generative Adversarial Networks (GANs) are at the forefront of AI innovation,\ndriving advancements in areas such as image synthesis, medical imaging, and\ndata augmentation. However, the unique computational operations within GANs,\nsuch as transposed convolutions and instance normalization, introduce\nsignificant inefficiencies when executed on traditional electronic\naccelerators, resulting in high energy consumption and suboptimal performance.\nTo address these challenges, we introduce PhotoGAN, the first silicon-photonic\naccelerator designed to handle the specialized operations of GAN models. By\nleveraging the inherent high throughput and energy efficiency of silicon\nphotonics, PhotoGAN offers an innovative, reconfigurable architecture capable\nof accelerating transposed convolutions and other GAN-specific layers. The\naccelerator also incorporates a sparse computation optimization technique to\nreduce redundant operations, improving computational efficiency. Our\nexperimental results demonstrate that PhotoGAN achieves at least 4.4x higher\nGOPS and 2.18x lower energy-per-bit (EPB) compared to state-of-the-art\naccelerators, including GPUs and TPUs. These findings showcase PhotoGAN as a\npromising solution for the next generation of GAN acceleration, providing\nsubstantial gains in both performance and energy efficiency.", "link": "http://arxiv.org/abs/2501.13828v1", "categories": ["cs.AR", "cs.LG"]}, {"title": "Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline\n  Professional Videos", "authors": ["Kairui Hu", "Penghao Wu", "Fanyi Pu", "Wang Xiao", "Yuanhan Zhang", "Xiang Yue", "Bo Li", "Ziwei Liu"], "published": "2025-01-23T16:51:47Z", "summary": "Humans acquire knowledge through three cognitive stages: perceiving\ninformation, comprehending knowledge, and adapting knowledge to solve novel\nproblems. Videos serve as an effective medium for this learning process,\nfacilitating a progression through these cognitive stages. However, existing\nvideo benchmarks fail to systematically evaluate the knowledge acquisition\ncapabilities in Large Multimodal Models (LMMs). To address this gap, we\nintroduce Video-MMMU, a multi-modal, multi-disciplinary benchmark designed to\nassess LMMs' ability to acquire and utilize knowledge from videos. Video-MMMU\nfeatures a curated collection of 300 expert-level videos and 900\nhuman-annotated questions across six disciplines, evaluating knowledge\nacquisition through stage-aligned question-answer pairs: Perception,\nComprehension, and Adaptation. A proposed knowledge gain metric,\n{\\Delta}knowledge, quantifies improvement in performance after video viewing.\nEvaluation of LMMs reveals a steep decline in performance as cognitive demands\nincrease and highlights a significant gap between human and model knowledge\nacquisition, underscoring the need for methods to enhance LMMs' capability to\nlearn and adapt from videos.", "link": "http://arxiv.org/abs/2501.13826v1", "categories": ["cs.CV", "cs.CL"]}, {"title": "Hallucinations Can Improve Large Language Models in Drug Discovery", "authors": ["Shuzhou Yuan", "Michael F\u00e4rber"], "published": "2025-01-23T16:45:51Z", "summary": "Concerns about hallucinations in Large Language Models (LLMs) have been\nraised by researchers, yet their potential in areas where creativity is vital,\nsuch as drug discovery, merits exploration. In this paper, we come up with the\nhypothesis that hallucinations can improve LLMs in drug discovery. To verify\nthis hypothesis, we use LLMs to describe the SMILES string of molecules in\nnatural language and then incorporate these descriptions as part of the prompt\nto address specific tasks in drug discovery. Evaluated on seven LLMs and five\nclassification tasks, our findings confirm the hypothesis: LLMs can achieve\nbetter performance with text containing hallucinations. Notably, Llama-3.1-8B\nachieves an 18.35% gain in ROC-AUC compared to the baseline without\nhallucination. Furthermore, hallucinations generated by GPT-4o provide the most\nconsistent improvements across models. Additionally, we conduct empirical\nanalyses and a case study to investigate key factors affecting performance and\nthe underlying reasons. Our research sheds light on the potential use of\nhallucinations for LLMs and offers new perspectives for future research\nleveraging LLMs in drug discovery.", "link": "http://arxiv.org/abs/2501.13824v1", "categories": ["cs.CL", "cs.AI"]}, {"title": "Consistent spectral clustering in sparse tensor block models", "authors": ["Ian V\u00e4limaa", "Lasse Leskel\u00e4"], "published": "2025-01-23T16:41:19Z", "summary": "High-order clustering aims to classify objects in multiway datasets that are\nprevalent in various fields such as bioinformatics, social network analysis,\nand recommendation systems. These tasks often involve data that is sparse and\nhigh-dimensional, presenting significant statistical and computational\nchallenges. This paper introduces a tensor block model specifically designed\nfor sparse integer-valued data tensors. We propose a simple spectral clustering\nalgorithm augmented with a trimming step to mitigate noise fluctuations, and\nidentify a density threshold that ensures the algorithm's consistency. Our\napproach models sparsity using a sub-Poisson noise concentration framework,\naccommodating heavier than sub-Gaussian tails. Remarkably, this natural class\nof tensor block models is closed under aggregation across arbitrary modes.\nConsequently, we obtain a comprehensive framework for evaluating the tradeoff\nbetween signal loss and noise reduction during data aggregation. The analysis\nis based on a novel concentration bound for sparse random Gram matrices. The\ntheoretical findings are illustrated through simulation experiments.", "link": "http://arxiv.org/abs/2501.13820v1", "categories": ["math.ST", "cs.LG", "math.PR", "stat.TH", "62H30"]}, {"title": "Ensuring Medical AI Safety: Explainable AI-Driven Detection and\n  Mitigation of Spurious Model Behavior and Associated Data", "authors": ["Frederik Pahde", "Thomas Wiegand", "Sebastian Lapuschkin", "Wojciech Samek"], "published": "2025-01-23T16:39:09Z", "summary": "Deep neural networks are increasingly employed in high-stakes medical\napplications, despite their tendency for shortcut learning in the presence of\nspurious correlations, which can have potentially fatal consequences in\npractice. Detecting and mitigating shortcut behavior is a challenging task that\noften requires significant labeling efforts from domain experts. To alleviate\nthis problem, we introduce a semi-automated framework for the identification of\nspurious behavior from both data and model perspective by leveraging insights\nfrom eXplainable Artificial Intelligence (XAI). This allows the retrieval of\nspurious data points and the detection of model circuits that encode the\nassociated prediction rules. Moreover, we demonstrate how these shortcut\nencodings can be used for XAI-based sample- and pixel-level data annotation,\nproviding valuable information for bias mitigation methods to unlearn the\nundesired shortcut behavior. We show the applicability of our framework using\nfour medical datasets across two modalities, featuring controlled and\nreal-world spurious correlations caused by data artifacts. We successfully\nidentify and mitigate these biases in VGG16, ResNet50, and contemporary Vision\nTransformer models, ultimately increasing their robustness and applicability\nfor real-world medical tasks.", "link": "http://arxiv.org/abs/2501.13818v1", "categories": ["cs.AI", "cs.CV", "cs.LG"]}, {"title": "Temporal Logic Guided Safe Navigation for Autonomous Vehicles", "authors": ["Aditya Parameshwaran", "Yue Wang"], "published": "2025-01-23T16:39:08Z", "summary": "Safety verification for autonomous vehicles (AVs) and ground robots is\ncrucial for ensuring reliable operation given their uncertain environments.\nFormal language tools provide a robust and sound method to verify safety rules\nfor such complex cyber-physical systems. In this paper, we propose a hybrid\napproach that combines the strengths of formal verification languages like\nLinear Temporal Logic (LTL) and Signal Temporal Logic (STL) to generate safe\ntrajectories and optimal control inputs for autonomous vehicle navigation. We\nimplement a symbolic path planning approach using LTL to generate a formally\nsafe reference trajectory. A mixed integer linear programming (MILP) solver is\nthen used on this reference trajectory to solve for the control inputs while\nsatisfying the state, control and safety constraints described by STL. We test\nour proposed solution on two environments and compare the results with popular\npath planning algorithms. In contrast to conventional path planning algorithms,\nour formally safe solution excels in handling complex specification scenarios\nwhile ensuring both safety and comparable computation times.", "link": "http://arxiv.org/abs/2501.13817v1", "categories": ["cs.RO", "cs.FL", "cs.SY", "eess.SY"]}, {"title": "By-Example Synthesis of Vector Textures", "authors": ["Christopher Palazzolo", "Oliver van Kaick", "David Mould"], "published": "2025-01-23T16:35:21Z", "summary": "We propose a new method for synthesizing an arbitrarily sized novel vector\ntexture given a single raster exemplar. Our method first segments the exemplar\nto extract the primary textons, and then clusters them based on visual\nsimilarity. We then compute a descriptor to capture each texton's neighborhood\nwhich contains the inter-category relationships that are used at synthesis\ntime. Next, we use a simple procedure to both extract and place the secondary\ntextons behind the primary polygons. Finally, our method constructs a gradient\nfield for the background which is defined by a set of data points and colors.\nThe color of the secondary polygons are also adjusted to better match the\ngradient field. To compare our work with other methods, we use a wide range of\nperceptual-based metrics.", "link": "http://arxiv.org/abs/2501.13812v1", "categories": ["cs.CV", "cs.GR"]}, {"title": "Learning to Help in Multi-Class Settings", "authors": ["Yu Wu", "Yansong Li", "Zeyu Dong", "Nitya Sathyavageeswaran", "Anand D. Sarwate"], "published": "2025-01-23T16:32:01Z", "summary": "Deploying complex machine learning models on resource-constrained devices is\nchallenging due to limited computational power, memory, and model\nretrainability. To address these limitations, a hybrid system can be\nestablished by augmenting the local model with a server-side model, where\nsamples are selectively deferred by a rejector and then sent to the server for\nprocessing. The hybrid system enables efficient use of computational resources\nwhile minimizing the overhead associated with server usage. The recently\nproposed Learning to Help (L2H) model trains a server model given a fixed local\n(client) model, differing from the Learning to Defer (L2D) framework, which\ntrains the client for a fixed (expert) server. In both L2D and L2H, the\ntraining includes learning a rejector at the client to determine when to query\nthe server. In this work, we extend the L2H model from binary to multi-class\nclassification problems and demonstrate its applicability in a number of\ndifferent scenarios of practical interest in which access to the server may be\nlimited by cost, availability, or policy. We derive a stage-switching surrogate\nloss function that is differentiable, convex, and consistent with the Bayes\nrule corresponding to the 0-1 loss for the L2H model. Experiments show that our\nproposed methods offer an efficient and practical solution for multi-class\nclassification in resource-constrained environments.", "link": "http://arxiv.org/abs/2501.13810v1", "categories": ["cs.LG", "cs.AI"]}, {"title": "Generation of reusable learning objects from digital medical\n  collections: An analysis based on the MASMDOA framework", "authors": ["F\u00e9lix Buend\u00eda", "Joaqu\u00edn Gayoso-Cabada", "Jos\u00e9-Luis Sierra"], "published": "2025-01-23T16:27:15Z", "summary": "Learning Objects represent a widespread approach to structuring instructional\nmaterials in a large variety of educational contexts. The main aim of this work\nconsists of analyzing from a qualitative point of view the process of\ngenerating reusable learning objects (RLOs) followed by Clavy, a tool that can\nbe used to retrieve data from multiple medical knowledge sources and\nreconfigure such sources in diverse multimedia-based structures and\norganizations. From these organizations, Clavy is able to generate learning\nobjects which can be adapted to various instructional healthcare scenarios with\nseveral types of user profiles and distinct learning requirements. Moreover,\nClavy provides the capability of exporting these learning objects through\neducational standard specifications, which improves their reusability features.\nThe analysis insights highlight the importance of having a tool able to\ntransfer knowledge from the available digital medical collections to learning\nobjects that can be easily accessed by medical students and healthcare\npractitioners through the most popular e-learning platforms.", "link": "http://arxiv.org/abs/2501.13806v1", "categories": ["cs.CL", "cs.HC"]}, {"title": "EgoHand: Ego-centric Hand Pose Estimation and Gesture Recognition with\n  Head-mounted Millimeter-wave Radar and IMUs", "authors": ["Yizhe Lv", "Tingting Zhang", "Yunpeng Song", "Han Ding", "Jinsong Han", "Fei Wang"], "published": "2025-01-23T16:25:08Z", "summary": "Recent advanced Virtual Reality (VR) headsets, such as the Apple Vision Pro,\nemploy bottom-facing cameras to detect hand gestures and inputs, which offers\nusers significant convenience in VR interactions. However, these bottom-facing\ncameras can sometimes be inconvenient and pose a risk of unintentionally\nexposing sensitive information, such as private body parts or personal\nsurroundings. To mitigate these issues, we introduce EgoHand. This system\nprovides an alternative solution by integrating millimeter-wave radar and IMUs\nfor hand gesture recognition, thereby offering users an additional option for\ngesture interaction that enhances privacy protection. To accurately recognize\nhand gestures, we devise a two-stage skeleton-based gesture recognition scheme.\nIn the first stage, a novel end-to-end Transformer architecture is employed to\nestimate the coordinates of hand joints. Subsequently, these estimated joint\ncoordinates are utilized for gesture recognition. Extensive experiments\ninvolving 10 subjects show that EgoHand can detect hand gestures with 90.8%\naccuracy. Furthermore, EgoHand demonstrates robust performance across a variety\nof cross-domain tests, including different users, dominant hands, body\npostures, and scenes.", "link": "http://arxiv.org/abs/2501.13805v1", "categories": ["cs.CV"]}, {"title": "Towards Real-World Validation of a Physics-Based Ship Motion Prediction\n  Model", "authors": ["Michail Mathioudakis", "Christos Papandreou", "Theodoros Stouraitis", "Vicky Margari", "Antonios Nikitakis", "Stavros Paschalakis", "Konstantinos Kyriakopoulos", "Kostas J. Spyrou"], "published": "2025-01-23T16:23:15Z", "summary": "The maritime industry aims towards a sustainable future, which requires\nsignificant improvements in operational efficiency. Current approaches focus on\nminimising fuel consumption and emissions through greater autonomy. Efficient\nand safe autonomous navigation requires high-fidelity ship motion models\napplicable to real-world conditions. Although physics-based ship motion models\ncan predict ships' motion with sub-second resolution, their validation in\nreal-world conditions is rarely found in the literature. This study presents a\nphysics-based 3D dynamics motion model that is tailored to a container-ship,\nand compares its predictions against real-world voyages. The model integrates\nvessel motion over time and accounts for its hydrodynamic behavior under\ndifferent environmental conditions. The model's predictions are evaluated\nagainst real vessel data both visually and using multiple distance measures.\nBoth methodologies demonstrate that the model's predictions align closely with\nthe real-world trajectories of the container-ship.", "link": "http://arxiv.org/abs/2501.13804v1", "categories": ["eess.SY", "cs.RO", "cs.SY"]}, {"title": "PromptMono: Cross Prompting Attention for Self-Supervised Monocular\n  Depth Estimation in Challenging Environments", "authors": ["Changhao Wang", "Guanwen Zhang", "Zhengyun Cheng", "Wei Zhou"], "published": "2025-01-23T16:14:02Z", "summary": "Considerable efforts have been made to improve monocular depth estimation\nunder ideal conditions. However, in challenging environments, monocular depth\nestimation still faces difficulties. In this paper, we introduce visual prompt\nlearning for predicting depth across different environments within a unified\nmodel, and present a self-supervised learning framework called PromptMono. It\nemploys a set of learnable parameters as visual prompts to capture\ndomain-specific knowledge. To integrate prompting information into image\nrepresentations, a novel gated cross prompting attention (GCPA) module is\nproposed, which enhances the depth estimation in diverse conditions. We\nevaluate the proposed PromptMono on the Oxford Robotcar dataset and the\nnuScenes dataset. Experimental results demonstrate the superior performance of\nthe proposed method.", "link": "http://arxiv.org/abs/2501.13796v1", "categories": ["cs.CV"]}, {"title": "Training-Free Zero-Shot Temporal Action Detection with Vision-Language\n  Models", "authors": ["Chaolei Han", "Hongsong Wang", "Jidong Kuang", "Lei Zhang", "Jie Gui"], "published": "2025-01-23T16:13:58Z", "summary": "Existing zero-shot temporal action detection (ZSTAD) methods predominantly\nuse fully supervised or unsupervised strategies to recognize unseen activities.\nHowever, these training-based methods are prone to domain shifts and require\nhigh computational costs, which hinder their practical applicability in\nreal-world scenarios. In this paper, unlike previous works, we propose a\ntraining-Free Zero-shot temporal Action Detection (FreeZAD) method, leveraging\nexisting vision-language (ViL) models to directly classify and localize unseen\nactivities within untrimmed videos without any additional fine-tuning or\nadaptation. We mitigate the need for explicit temporal modeling and reliance on\npseudo-label quality by designing the LOGarithmic decay weighted\nOuter-Inner-Contrastive Score (LogOIC) and frequency-based Actionness\nCalibration. Furthermore, we introduce a test-time adaptation (TTA) strategy\nusing Prototype-Centric Sampling (PCS) to expand FreeZAD, enabling ViL models\nto adapt more effectively for ZSTAD. Extensive experiments on the THUMOS14 and\nActivityNet-1.3 datasets demonstrate that our training-free method outperforms\nstate-of-the-art unsupervised methods while requiring only 1/13 of the runtime.\nWhen equipped with TTA, the enhanced method further narrows the gap with fully\nsupervised methods.", "link": "http://arxiv.org/abs/2501.13795v1", "categories": ["cs.CV"]}, {"title": "Unveiling the Power of Noise Priors: Enhancing Diffusion Models for\n  Mobile Traffic Prediction", "authors": ["Zhi Sheng", "Yuan Yuan", "Jingtao Ding", "Yong Li"], "published": "2025-01-23T16:13:08Z", "summary": "Accurate prediction of mobile traffic, \\textit{i.e.,} network traffic from\ncellular base stations, is crucial for optimizing network performance and\nsupporting urban development. However, the non-stationary nature of mobile\ntraffic, driven by human activity and environmental changes, leads to both\nregular patterns and abrupt variations. Diffusion models excel in capturing\nsuch complex temporal dynamics due to their ability to capture the inherent\nuncertainties. Most existing approaches prioritize designing novel denoising\nnetworks but often neglect the critical role of noise itself, potentially\nleading to sub-optimal performance. In this paper, we introduce a novel\nperspective by emphasizing the role of noise in the denoising process. Our\nanalysis reveals that noise fundamentally shapes mobile traffic predictions,\nexhibiting distinct and consistent patterns. We propose NPDiff, a framework\nthat decomposes noise into \\textit{prior} and \\textit{residual} components,\nwith the \\textit{prior} derived from data dynamics, enhancing the model's\nability to capture both regular and abrupt variations. NPDiff can seamlessly\nintegrate with various diffusion-based prediction models, delivering\npredictions that are effective, efficient, and robust. Extensive experiments\ndemonstrate that it achieves superior performance with an improvement over\n30\\%, offering a new perspective on leveraging diffusion models in this domain.", "link": "http://arxiv.org/abs/2501.13794v1", "categories": ["cs.LG"]}, {"title": "Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic\n  Regression", "authors": ["Michael Crawshaw", "Blake Woodworth", "Mingrui Liu"], "published": "2025-01-23T16:09:26Z", "summary": "We analyze two variants of Local Gradient Descent applied to distributed\nlogistic regression with heterogeneous, separable data and show convergence at\nthe rate $O(1/KR)$ for $K$ local steps and sufficiently large $R$ communication\nrounds. In contrast, all existing convergence guarantees for Local GD applied\nto any problem are at least $\\Omega(1/R)$, meaning they fail to show the\nbenefit of local updates. The key to our improved guarantee is showing progress\non the logistic regression objective when using a large stepsize $\\eta \\gg\n1/K$, whereas prior analysis depends on $\\eta \\leq 1/K$.", "link": "http://arxiv.org/abs/2501.13790v1", "categories": ["cs.LG"]}, {"title": "Parameter-Efficient Fine-Tuning for Foundation Models", "authors": ["Dan Zhang", "Tao Feng", "Lilong Xue", "Yuandong Wang", "Yuxiao Dong", "Jie Tang"], "published": "2025-01-23T16:04:23Z", "summary": "This survey delves into the realm of Parameter-Efficient Fine-Tuning (PEFT)\nwithin the context of Foundation Models (FMs). PEFT, a cost-effective\nfine-tuning technique, minimizes parameters and computational complexity while\nstriving for optimal downstream task performance. FMs, like ChatGPT, DALL-E,\nand LLaVA specialize in language understanding, generative tasks, and\nmultimodal tasks, trained on diverse datasets spanning text, images, and\nvideos. The diversity of FMs guides various adaptation strategies for PEFT.\nTherefore, this survey aims to provide a comprehensive overview of PEFT\ntechniques applied to diverse FMs and address critical gaps in understanding\nthe techniques, trends, and applications. We start by providing a detailed\ndevelopment of FMs and PEFT. Subsequently, we systematically review the key\ncategories and core mechanisms of PEFT across diverse FMs to offer a\ncomprehensive understanding of trends. We also explore the most recent\napplications across various FMs to demonstrate the versatility of PEFT,\nshedding light on the integration of systematic PEFT methods with a range of\nFMs. Furthermore, we identify potential research and development directions for\nimproving PEFTs in the future. This survey provides a valuable resource for\nboth newcomers and experts seeking to understand and use the power of PEFT\nacross FMs. All reviewed papers are listed at\n\\url{https://github.com/THUDM/Awesome-Parameter-Efficient-Fine-Tuning-for-Foundation-Models}.", "link": "http://arxiv.org/abs/2501.13787v1", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, {"title": "Fast Iterative and Task-Specific Imputation with Online Learning", "authors": ["Rahul Bordoloi", "Cl\u00e9mence R\u00e9da", "Saptarshi Bej"], "published": "2025-01-23T16:04:18Z", "summary": "Missing feature values are a significant hurdle for downstream\nmachine-learning tasks such as classification and regression. However, they are\npervasive in multiple real-life use cases, for instance, in drug discovery\nresearch. Moreover, imputation methods might be time-consuming and offer few\nguarantees on the imputation quality, especially for not-missing-at-random\nmechanisms. We propose an imputation approach named F3I based on the iterative\nimprovement of a K-nearest neighbor imputation that learns the weights for each\nneighbor of a data point, optimizing for the most likely distribution of points\nover data points. This algorithm can also be jointly trained with a downstream\ntask on the imputed values. We provide a theoretical analysis of the imputation\nquality by F3I for several types of missing mechanisms. We also demonstrate the\nperformance of F3I on both synthetic data sets and real-life drug repurposing\nand handwritten-digit recognition data.", "link": "http://arxiv.org/abs/2501.13786v1", "categories": ["cs.LG"]}, {"title": "Defending against Adversarial Malware Attacks on ML-based Android\n  Malware Detection Systems", "authors": ["Ping He", "Lorenzo Cavallaro", "Shouling Ji"], "published": "2025-01-23T15:59:01Z", "summary": "Android malware presents a persistent threat to users' privacy and data\nintegrity. To combat this, researchers have proposed machine learning-based\n(ML-based) Android malware detection (AMD) systems. However, adversarial\nAndroid malware attacks compromise the detection integrity of the ML-based AMD\nsystems, raising significant concerns. Existing defenses against adversarial\nAndroid malware provide protections against feature space attacks which\ngenerate adversarial feature vectors only, leaving protection against realistic\nthreats from problem space attacks which generate real adversarial malware an\nopen problem. In this paper, we address this gap by proposing ADD, a practical\nadversarial Android malware defense framework designed as a plug-in to enhance\nthe adversarial robustness of the ML-based AMD systems against problem space\nattacks. Our extensive evaluation across various ML-based AMD systems\ndemonstrates that ADD is effective against state-of-the-art problem space\nadversarial Android malware attacks. Additionally, ADD shows the defense\neffectiveness in enhancing the adversarial robustness of real-world antivirus\nsolutions.", "link": "http://arxiv.org/abs/2501.13782v1", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"]}, {"title": "Matrix Completion in Group Testing: Bounds and Simulations", "authors": ["Trung-Khang Tran", "Thach V. Bui"], "published": "2025-01-23T15:58:30Z", "summary": "The main goal of group testing is to identify a small number of defective\nitems in a large population of items. A test on a subset of items is positive\nif the subset contains at least one defective item and negative otherwise. In\nnon-adaptive design, all tests can be tested simultaneously and represented by\na measurement matrix in which a row and a column represent a test and an item,\nrespectively. An entry in row $i$ and column $j$ is 1 if item $j$ belongs to\nthe test $i$ and is 0 otherwise. Given an unknown set of defective items, the\nobjective is to design a measurement matrix such that, by observing its\ncorresponding outcome vector, the defective items can be recovered efficiently.\nThe basic trait of this approach is that the measurement matrix has remained\nunchanged throughout the course of generating the outcome vector and recovering\ndefective items. In this paper, we study the case in which some entries in the\nmeasurement matrix are erased, called \\emph{the missing measurement matrix},\nbefore the recovery phase of the defective items, and our objective is to fully\nrecover the measurement matrix from the missing measurement matrix. In\nparticular, we show that some specific rows with erased entries provide\ninformation aiding the recovery while others do not. Given measurement matrices\nand erased entries follow the Bernoulli distribution, we show that before the\nerasing event happens, sampling sufficient sets of defective items and their\ncorresponding outcome vectors can help us recover the measurement matrix from\nthe missing measurement matrix.", "link": "http://arxiv.org/abs/2501.13780v1", "categories": ["cs.IT", "cs.LG", "math.IT"]}, {"title": "Not Every AI Problem is a Data Problem: We Should Be Intentional About\n  Data Scaling", "authors": ["Tanya Rodchenko", "Natasha Noy", "Nino Scherrer", "Jennifer Prendki"], "published": "2025-01-23T15:58:14Z", "summary": "While Large Language Models require more and more data to train and scale,\nrather than looking for any data to acquire, we should consider what types of\ntasks are more likely to benefit from data scaling. We should be intentional in\nour data acquisition. We argue that the topology of data itself informs which\ntasks to prioritize in data scaling, and shapes the development of the next\ngeneration of compute paradigms for tasks where data scaling is inefficient, or\neven insufficient.", "link": "http://arxiv.org/abs/2501.13779v1", "categories": ["cs.LG", "cs.AI"]}, {"title": "Explainable XR: Understanding User Behaviors of XR Environments using\n  LLM-assisted Analytics Framework", "authors": ["Yoonsang Kim", "Zainab Aamir", "Mithilesh Singh", "Saeed Boorboor", "Klaus Mueller", "Arie E. Kaufman"], "published": "2025-01-23T15:55:07Z", "summary": "We present Explainable XR, an end-to-end framework for analyzing user\nbehavior in diverse eXtended Reality (XR) environments by leveraging Large\nLanguage Models (LLMs) for data interpretation assistance. Existing XR user\nanalytics frameworks face challenges in handling cross-virtuality - AR, VR, MR\n- transitions, multi-user collaborative application scenarios, and the\ncomplexity of multimodal data. Explainable XR addresses these challenges by\nproviding a virtuality-agnostic solution for the collection, analysis, and\nvisualization of immersive sessions. We propose three main components in our\nframework: (1) A novel user data recording schema, called User Action\nDescriptor (UAD), that can capture the users' multimodal actions, along with\ntheir intents and the contexts; (2) a platform-agnostic XR session recorder,\nand (3) a visual analytics interface that offers LLM-assisted insights tailored\nto the analysts' perspectives, facilitating the exploration and analysis of the\nrecorded XR session data. We demonstrate the versatility of Explainable XR by\ndemonstrating five use-case scenarios, in both individual and collaborative XR\napplications across virtualities. Our technical evaluation and user studies\nshow that Explainable XR provides a highly usable analytics solution for\nunderstanding user actions and delivering multifaceted, actionable insights\ninto user behaviors in immersive environments.", "link": "http://arxiv.org/abs/2501.13778v1", "categories": ["cs.HC", "cs.CL"]}, {"title": "Crossfire: An Elastic Defense Framework for Graph Neural Networks Under\n  Bit Flip Attacks", "authors": ["Lorenz Kummer", "Samir Moustafa", "Wilfried Gansterer", "Nils Kriege"], "published": "2025-01-23T15:53:35Z", "summary": "Bit Flip Attacks (BFAs) are a well-established class of adversarial attacks,\noriginally developed for Convolutional Neural Networks within the computer\nvision domain. Most recently, these attacks have been extended to target Graph\nNeural Networks (GNNs), revealing significant vulnerabilities. This new\ndevelopment naturally raises questions about the best strategies to defend GNNs\nagainst BFAs, a challenge for which no solutions currently exist. Given the\napplications of GNNs in critical fields, any defense mechanism must not only\nmaintain network performance, but also verifiably restore the network to its\npre-attack state. Verifiably restoring the network to its pre-attack state also\neliminates the need for costly evaluations on test data to ensure network\nquality. We offer first insights into the effectiveness of existing honeypot-\nand hashing-based defenses against BFAs adapted from the computer vision domain\nto GNNs, and characterize the shortcomings of these approaches. To overcome\ntheir limitations, we propose Crossfire, a hybrid approach that exploits weight\nsparsity and combines hashing and honeypots with bit-level correction of\nout-of-distribution weight elements to restore network integrity. Crossfire is\nretraining-free and does not require labeled data. Averaged over 2,160\nexperiments on six benchmark datasets, Crossfire offers a 21.8% higher\nprobability than its competitors of reconstructing a GNN attacked by a BFA to\nits pre-attack state. These experiments cover up to 55 bit flips from various\nattacks. Moreover, it improves post-repair prediction quality by 10.85%.\nComputational and storage overheads are negligible compared to the inherent\ncomplexity of even the simplest GNNs.", "link": "http://arxiv.org/abs/2501.13776v1", "categories": ["cs.LG"]}, {"title": "Do Large Language Models Truly Understand Geometric Structures?", "authors": ["Xiaofeng Wang", "Yiming Wang", "Wenhong Zhu", "Rui Wang"], "published": "2025-01-23T15:52:34Z", "summary": "Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements.", "link": "http://arxiv.org/abs/2501.13773v1", "categories": ["cs.CL"]}, {"title": "Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits\n  on Large Audio Language Models in Jailbreak", "authors": ["Erjia Xiao", "Hao Cheng", "Jing Shao", "Jinhao Duan", "Kaidi Xu", "Le Yang", "Jindong Gu", "Renjing Xu"], "published": "2025-01-23T15:51:38Z", "summary": "Large Language Models (LLMs) demonstrate remarkable zero-shot performance\nacross various natural language processing tasks. The integration of multimodal\nencoders extends their capabilities, enabling the development of Multimodal\nLarge Language Models that process vision, audio, and text. However, these\ncapabilities also raise significant security concerns, as these models can be\nmanipulated to generate harmful or inappropriate content through jailbreak.\nWhile extensive research explores the impact of modality-specific input edits\non text-based LLMs and Large Vision-Language Models in jailbreak, the effects\nof audio-specific edits on Large Audio-Language Models (LALMs) remain\nunderexplored. Hence, this paper addresses this gap by investigating how\naudio-specific edits influence LALMs inference regarding jailbreak. We\nintroduce the Audio Editing Toolbox (AET), which enables audio-modality edits\nsuch as tone adjustment, word emphasis, and noise injection, and the Edited\nAudio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also\nconduct extensive evaluations of state-of-the-art LALMs to assess their\nrobustness under different audio edits. This work lays the groundwork for\nfuture explorations on audio-modality interactions in LALMs security.", "link": "http://arxiv.org/abs/2501.13772v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"]}, {"title": "An Efficient Diffusion-based Non-Autoregressive Solver for Traveling\n  Salesman Problem", "authors": ["Mingzhao Wang", "You Zhou", "Zhiguang Cao", "Yubin Xiao", "Xuan Wu", "Wei Pang", "Yuan Jiang", "Hui Yang", "Peng Zhao", "Yuanshu Li"], "published": "2025-01-23T15:47:04Z", "summary": "Recent advances in neural models have shown considerable promise in solving\nTraveling Salesman Problems (TSPs) without relying on much hand-crafted\nengineering. However, while non-autoregressive (NAR) approaches benefit from\nfaster inference through parallelism, they typically deliver solutions of\ninferior quality compared to autoregressive ones. To enhance the solution\nquality while maintaining fast inference, we propose DEITSP, a diffusion model\nwith efficient iterations tailored for TSP that operates in a NAR manner.\nFirstly, we introduce a one-step diffusion model that integrates the controlled\ndiscrete noise addition process with self-consistency enhancement, enabling\noptimal solution prediction through simultaneous denoising of multiple\nsolutions. Secondly, we design a dual-modality graph transformer to bolster the\nextraction and fusion of features from node and edge modalities, while further\naccelerating the inference with fewer layers. Thirdly, we develop an efficient\niterative strategy that alternates between adding and removing noise to improve\nexploration compared to previous diffusion methods. Additionally, we devise a\nscheduling framework to progressively refine the solution space by adjusting\nnoise levels, facilitating a smooth search for optimal solutions. Extensive\nexperiments on real-world and large-scale TSP instances demonstrate that DEITSP\nperforms favorably against existing neural approaches in terms of solution\nquality, inference latency, and generalization ability. Our code is available\nat $\\href{https://github.com/DEITSP/DEITSP}{https://github.com/DEITSP/DEITSP}$.", "link": "http://arxiv.org/abs/2501.13767v1", "categories": ["cs.LG"]}, {"title": "UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level\n  Mathematical Reasoning with Large Language Models", "authors": ["Xin Xu", "Jiaxin Zhang", "Tianhao Chen", "Zitong Chao", "Jishan Hu", "Can Yang"], "published": "2025-01-23T15:46:43Z", "summary": "Large Language Models (LLMs) have made significant strides in mathematical\nreasoning, underscoring the need for a comprehensive and fair evaluation of\ntheir capabilities. However, existing benchmarks often fall short, either\nlacking extensive coverage of undergraduate-level mathematical problems or\nprobably suffering from test-set contamination. To address these issues, we\nintroduce UGMathBench, a diverse and dynamic benchmark specifically designed\nfor evaluating undergraduate-level mathematical reasoning with LLMs.\nUGMathBench comprises 5,062 problems across 16 subjects and 111 topics,\nfeaturing 10 distinct answer types. Each problem includes three randomized\nversions, with additional versions planned for release as leading open-source\nLLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:\neffective accuracy (EAcc), which measures the percentage of correctly solved\nproblems across all three versions, and reasoning gap ($\\Delta$), which\nassesses reasoning robustness by calculating the difference between the average\naccuracy across all versions and EAcc. Our extensive evaluation of 23 leading\nLLMs reveals that the highest EAcc achieved is 56.3\\% by OpenAI-o1-mini, with\nlarge $\\Delta$ values observed across different models. This highlights the\nneed for future research aimed at developing \"large reasoning models\" with high\nEAcc and $\\Delta = 0$. We anticipate that the release of UGMathBench, along\nwith its detailed evaluation codes, will serve as a valuable resource to\nadvance the development of LLMs in solving mathematical problems.", "link": "http://arxiv.org/abs/2501.13766v1", "categories": ["cs.CL", "cs.AI"]}, {"title": "Integrating Causality with Neurochaos Learning: Proposed Approach and\n  Research Agenda", "authors": ["Nanjangud C. Narendra", "Nithin Nagaraj"], "published": "2025-01-23T15:45:29Z", "summary": "Deep learning implemented via neural networks, has revolutionized machine\nlearning by providing methods for complex tasks such as object\ndetection/classification and prediction. However, architectures based on deep\nneural networks have started to yield diminishing returns, primarily due to\ntheir statistical nature and inability to capture causal structure in the\ntraining data. Another issue with deep learning is its high energy consumption,\nwhich is not that desirable from a sustainability perspective.\n  Therefore, alternative approaches are being considered to address these\nissues, both of which are inspired by the functioning of the human brain. One\napproach is causal learning, which takes into account causality among the items\nin the dataset on which the neural network is trained. It is expected that this\nwill help minimize the spurious correlations that are prevalent in the learned\nrepresentations of deep neural networks. The other approach is Neurochaos\nLearning, a recent development, which draws its inspiration from the nonlinear\nchaotic firing intrinsic to neurons in biological neural networks\n(brain/central nervous system). Both approaches have shown improved results\nover just deep learning alone.\n  To that end, in this position paper, we investigate how causal and neurochaos\nlearning approaches can be integrated together to produce better results,\nespecially in domains that contain linked data. We propose an approach for this\nintegration to enhance classification, prediction and reinforcement learning.\nWe also propose a set of research questions that need to be investigated in\norder to make this integration a reality.", "link": "http://arxiv.org/abs/2501.13763v1", "categories": ["cs.LG", "cs.AI", "I.2.6"]}, {"title": "On Deciding the Data Complexity of Answering Linear Monadic Datalog\n  Queries with LTL Operators(Extended Version)", "authors": ["Alessandro Artale", "Anton Gnatenko", "Vladislav Ryzhikov", "Michael Zakharyaschev"], "published": "2025-01-23T15:41:48Z", "summary": "Our concern is the data complexity of answering linear monadic datalog\nqueries whose atoms in the rule bodies can be prefixed by operators of linear\ntemporal logic LTL. We first observe that, for data complexity, answering any\nconnected query with operators $\\bigcirc/\\bigcirc^-$ (at the next/previous\nmoment) is either in AC0, or in $ACC0\\!\\setminus\\!AC0$, or $NC^1$-complete, or\nLogSpace-hard and in NLogSpace. Then we show that the problem of deciding\nLogSpace-hardness of answering such queries is PSpace-complete, while checking\nmembership in the classes AC0 and ACC0 as well as $NC^1$-completeness can be\ndone in ExpSpace. Finally, we prove that membership in AC0 or in ACC0,\n$NC^1$-completeness, and LogSpace-hardness are undecidable for queries with\noperators $\\Diamond_f/\\Diamond_p$ (sometime in the future/past) provided that\n$NC^1 \\ne NLogSpace$, and $LogSpace \\ne NLogSpace$.", "link": "http://arxiv.org/abs/2501.13762v1", "categories": ["cs.AI", "cs.CC", "cs.LO"]}, {"title": "2-Tier SimCSE: Elevating BERT for Robust Sentence Embeddings", "authors": ["Yumeng Wang", "Ziran Zhou", "Junjin Wang"], "published": "2025-01-23T15:36:35Z", "summary": "Effective sentence embeddings that capture semantic nuances and generalize\nwell across diverse contexts are crucial for natural language processing tasks.\nWe address this challenge by applying SimCSE (Simple Contrastive Learning of\nSentence Embeddings) using contrastive learning to fine-tune the minBERT model\nfor sentiment analysis, semantic textual similarity (STS), and paraphrase\ndetection. Our contributions include experimenting with three different dropout\ntechniques, namely standard dropout, curriculum dropout, and adaptive dropout,\nto tackle overfitting, proposing a novel 2-Tier SimCSE Fine-tuning Model that\ncombines both unsupervised and supervised SimCSE on STS task, and exploring\ntransfer learning potential for Paraphrase and SST tasks. Our findings\ndemonstrate the effectiveness of SimCSE, with the 2-Tier model achieving\nsuperior performance on the STS task, with an average test score of 0.742\nacross all three downstream tasks. The results of error analysis reveals\nchallenges in handling complex sentiments and reliance on lexical overlap for\nparaphrase detection, highlighting areas for future research. The ablation\nstudy revealed that removing Adaptive Dropout in the Single-Task Unsupervised\nSimCSE Model led to improved performance on the STS task, indicating\noverfitting due to added parameters. Transfer learning from SimCSE models on\nParaphrase and SST tasks did not enhance performance, suggesting limited\ntransferability of knowledge from the STS task.", "link": "http://arxiv.org/abs/2501.13758v1", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, {"title": "Solving the long-tailed distribution problem by exploiting the synergies\n  and balance of different techniques", "authors": ["Ziheng Wang", "Toni Lassila", "Sharib Ali"], "published": "2025-01-23T15:35:15Z", "summary": "In real-world data, long-tailed data distribution is common, making it\nchallenging for models trained on empirical risk minimisation to learn and\nclassify tail classes effectively. While many studies have sought to improve\nlong tail recognition by altering the data distribution in the feature space\nand adjusting model decision boundaries, research on the synergy and corrective\napproach among various methods is limited. Our study delves into three\nlong-tail recognition techniques: Supervised Contrastive Learning (SCL),\nRare-Class Sample Generator (RSG), and Label-Distribution-Aware Margin Loss\n(LDAM). SCL enhances intra-class clusters based on feature similarity and\npromotes clear inter-class separability but tends to favour dominant classes\nonly. When RSG is integrated into the model, we observed that the intra-class\nfeatures further cluster towards the class centre, which demonstrates a\nsynergistic effect together with SCL's principle of enhancing intra-class\nclustering. RSG generates new tail features and compensates for the tail\nfeature space squeezed by SCL. Similarly, LDAM is known to introduce a larger\nmargin specifically for tail classes; we demonstrate that LDAM further bolsters\nthe model's performance on tail classes when combined with the more explicit\ndecision boundaries achieved by SCL and RSG. Furthermore, SCL can compensate\nfor the dominant class accuracy sacrificed by RSG and LDAM. Our research\nemphasises the synergy and balance among the three techniques, with each\namplifying the strengths of the others and mitigating their shortcomings. Our\nexperiment on long-tailed distribution datasets, using an end-to-end\narchitecture, yields competitive results by enhancing tail class accuracy\nwithout compromising dominant class performance, achieving a balanced\nimprovement across all classes.", "link": "http://arxiv.org/abs/2501.13756v1", "categories": ["cs.CV", "cs.AI", "cs.LG"]}, {"title": "On Disentangled Training for Nonlinear Transform in Learned Image\n  Compression", "authors": ["Han Li", "Shaohui Li", "Wenrui Dai", "Maida Cao", "Nuowen Kan", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "published": "2025-01-23T15:32:06Z", "summary": "Learned image compression (LIC) has demonstrated superior rate-distortion\n(R-D) performance compared to traditional codecs, but is challenged by training\ninefficiency that could incur more than two weeks to train a state-of-the-art\nmodel from scratch. Existing LIC methods overlook the slow convergence caused\nby compacting energy in learning nonlinear transforms. In this paper, we first\nreveal that such energy compaction consists of two components, i.e., feature\ndecorrelation and uneven energy modulation. On such basis, we propose a linear\nauxiliary transform (AuxT) to disentangle energy compaction in training\nnonlinear transforms. The proposed AuxT obtains coarse approximation to achieve\nefficient energy compaction such that distribution fitting with the nonlinear\ntransforms can be simplified to fine details. We then develop wavelet-based\nlinear shortcuts (WLSs) for AuxT that leverages wavelet-based downsampling and\northogonal linear projection for feature decorrelation and subband-aware\nscaling for uneven energy modulation. AuxT is lightweight and plug-and-play to\nbe integrated into diverse LIC models to address the slow convergence issue.\nExperimental results demonstrate that the proposed approach can accelerate\ntraining of LIC models by 2 times and simultaneously achieves an average 1\\%\nBD-rate reduction. To our best knowledge, this is one of the first successful\nattempt that can significantly improve the convergence of LIC with comparable\nor superior rate-distortion performance. Code will be released at\n\\url{https://github.com/qingshi9974/AuxT}", "link": "http://arxiv.org/abs/2501.13751v1", "categories": ["eess.IV", "cs.CV"]}, {"title": "Exact Soft Analytical Side-Channel Attacks using Tractable Circuits", "authors": ["Thomas Wedenig", "Rishub Nagpal", "Ga\u00ebtan Cassiers", "Stefan Mangard", "Robert Peharz"], "published": "2025-01-23T15:25:40Z", "summary": "Detecting weaknesses in cryptographic algorithms is of utmost importance for\ndesigning secure information systems. The state-of-the-art soft analytical\nside-channel attack (SASCA) uses physical leakage information to make\nprobabilistic predictions about intermediate computations and combines these\n\"guesses\" with the known algorithmic logic to compute the posterior\ndistribution over the key. This attack is commonly performed via loopy belief\npropagation, which, however, lacks guarantees in terms of convergence and\ninference quality. In this paper, we develop a fast and exact inference method\nfor SASCA, denoted as ExSASCA, by leveraging knowledge compilation and\ntractable probabilistic circuits. When attacking the Advanced Encryption\nStandard (AES), the most widely used encryption algorithm to date, ExSASCA\noutperforms SASCA by more than 31% top-1 success rate absolute. By leveraging\nsparse belief messages, this performance is achieved with little more\ncomputational cost than SASCA, and about 3 orders of magnitude less than exact\ninference via exhaustive enumeration. Even with dense belief messages, ExSASCA\nstill uses 6 times less computations than exhaustive inference.", "link": "http://arxiv.org/abs/2501.13748v1", "categories": ["cs.LG", "cs.CR"]}, {"title": "EICopilot: Search and Explore Enterprise Information over Large-scale\n  Knowledge Graphs with LLM-driven Agents", "authors": ["Yuhui Yun", "Huilong Ye", "Xinru Li", "Ruojia Li", "Jingfeng Deng", "Li Li", "Haoyi Xiong"], "published": "2025-01-23T15:22:25Z", "summary": "The paper introduces EICopilot, an novel agent-based solution enhancing\nsearch and exploration of enterprise registration data within extensive online\nknowledge graphs like those detailing legal entities, registered capital, and\nmajor shareholders. Traditional methods necessitate text-based queries and\nmanual subgraph explorations, often resulting in time-consuming processes.\nEICopilot, deployed as a chatbot via Baidu Enterprise Search, improves this\nlandscape by utilizing Large Language Models (LLMs) to interpret natural\nlanguage queries. This solution automatically generates and executes Gremlin\nscripts, providing efficient summaries of complex enterprise relationships.\nDistinct feature a data pre-processing pipeline that compiles and annotates\nrepresentative queries into a vector database of examples for In-context\nlearning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thought\nwith ICL to enhance Gremlin script generation for knowledge graph search and\nexploration, and a novel query masking strategy that improves intent\nrecognition for heightened script accuracy. Empirical evaluations demonstrate\nthe superior performance of EICopilot, including speed and accuracy, over\nbaseline methods, with the \\emph{Full Mask} variant achieving a syntax error\nrate reduction to as low as 10.00% and an execution correctness of up to\n82.14%. These components collectively contribute to superior querying\ncapabilities and summarization of intricate datasets, positioning EICopilot as\na groundbreaking tool in the exploration and exploitation of large-scale\nknowledge graphs for enterprise information search.", "link": "http://arxiv.org/abs/2501.13746v1", "categories": ["cs.IR", "cs.AI"]}, {"title": "GPT-HTree: A Decision Tree Framework Integrating Hierarchical Clustering\n  and Large Language Models for Explainable Classification", "authors": ["Te Pei", "Fuat Alican", "Aaron Ontoyin Yin", "Yigit Ihlamur"], "published": "2025-01-23T15:18:22Z", "summary": "This paper introduces GPT-HTree, a framework combining hierarchical\nclustering, decision trees, and large language models (LLMs) to address this\nchallenge. By leveraging hierarchical clustering to segment individuals based\non salient features, resampling techniques to balance class distributions, and\ndecision trees to tailor classification paths within each cluster, GPT-HTree\nensures both accuracy and interpretability. LLMs enhance the framework by\ngenerating human-readable cluster descriptions, bridging quantitative analysis\nwith actionable insights.", "link": "http://arxiv.org/abs/2501.13743v1", "categories": ["cs.LG"]}, {"title": "A Study of the Plausibility of Attention between RNN Encoders in Natural\n  Language Inference", "authors": ["Duc Hau Nguyen", "Duc Hau Nguyen", "Pascale S\u00e9billot"], "published": "2025-01-23T15:11:27Z", "summary": "Attention maps in neural models for NLP are appealing to explain the decision\nmade by a model, hopefully emphasizing words that justify the decision. While\nmany empirical studies hint that attention maps can provide such justification\nfrom the analysis of sound examples, only a few assess the plausibility of\nexplanations based on attention maps, i.e., the usefulness of attention maps\nfor humans to understand the decision. These studies furthermore focus on text\nclassification. In this paper, we report on a preliminary assessment of\nattention maps in a sentence comparison task, namely natural language\ninference. We compare the cross-attention weights between two RNN encoders with\nhuman-based and heuristic-based annotations on the eSNLI corpus. We show that\nthe heuristic reasonably correlates with human annotations and can thus\nfacilitate evaluation of plausible explanations in sentence comparison tasks.\nRaw attention weights however remain only loosely related to a plausible\nexplanation.", "link": "http://arxiv.org/abs/2501.13735v1", "categories": ["cs.CL"]}, {"title": "Sample complexity of data-driven tuning of model hyperparameters in\n  neural networks with structured parameter-dependent dual function", "authors": ["Maria-Florina Balcan", "Anh Tuan Nguyen", "Dravyansh Sharma"], "published": "2025-01-23T15:10:51Z", "summary": "Modern machine learning algorithms, especially deep learning based\ntechniques, typically involve careful hyperparameter tuning to achieve the best\nperformance. Despite the surge of intense interest in practical techniques like\nBayesian optimization and random search based approaches to automating this\nlaborious and compute-intensive task, the fundamental learning theoretic\ncomplexity of tuning hyperparameters for deep neural networks is poorly\nunderstood. Inspired by this glaring gap, we initiate the formal study of\nhyperparameter tuning complexity in deep learning through a recently introduced\ndata driven setting. We assume that we have a series of deep learning tasks,\nand we have to tune hyperparameters to do well on average over the distribution\nof tasks. A major difficulty is that the utility function as a function of the\nhyperparameter is very volatile and furthermore, it is given implicitly by an\noptimization problem over the model parameters. This is unlike previous work in\ndata driven design, where one can typically explicitly model the algorithmic\nbehavior as a function of the hyperparameters. To tackle this challenge, we\nintroduce a new technique to characterize the discontinuities and oscillations\nof the utility function on any fixed problem instance as we vary the\nhyperparameter, our analysis relies on subtle concepts including tools from\ndifferential/algebraic geometry and constrained optimization. This can be used\nto show that the learning theoretic complexity of the corresponding family of\nutility functions is bounded. We instantiate our results and provide sample\ncomplexity bounds for concrete applications tuning a hyperparameter that\ninterpolates neural activation functions and setting the kernel parameter in\ngraph neural networks.", "link": "http://arxiv.org/abs/2501.13734v1", "categories": ["cs.LG"]}, {"title": "A dimensionality reduction technique based on the Gromov-Wasserstein\n  distance", "authors": ["Rafael P. Eufrazio", "Eduardo Fernandes Montesuma", "Charles C. Cavalcante"], "published": "2025-01-23T15:05:51Z", "summary": "Analyzing relationships between objects is a pivotal problem within data\nscience. In this context, Dimensionality reduction (DR) techniques are employed\nto generate smaller and more manageable data representations. This paper\nproposes a new method for dimensionality reduction, based on optimal\ntransportation theory and the Gromov-Wasserstein distance. We offer a new\nprobabilistic view of the classical Multidimensional Scaling (MDS) algorithm\nand the nonlinear dimensionality reduction algorithm, Isomap (Isometric Mapping\nor Isometric Feature Mapping) that extends the classical MDS, in which we use\nthe Gromov-Wasserstein distance between the probability measure of\nhigh-dimensional data, and its low-dimensional representation. Through gradient\ndescent, our method embeds high-dimensional data into a lower-dimensional\nspace, providing a robust and efficient solution for analyzing complex\nhigh-dimensional datasets.", "link": "http://arxiv.org/abs/2501.13732v1", "categories": ["stat.ML", "cs.LG"]}, {"title": "Pseudocode-Injection Magic: Enabling LLMs to Tackle Graph Computational\n  Tasks", "authors": ["Chang Gong", "Wanrui Bian", "Zhijie Zhang", "Weiguo Zheng"], "published": "2025-01-23T15:04:22Z", "summary": "Graph computational tasks are inherently challenging and often demand the\ndevelopment of advanced algorithms for effective solutions. With the emergence\nof large language models (LLMs), researchers have begun investigating their\npotential to address these tasks. However, existing approaches are constrained\nby LLMs' limited capability to comprehend complex graph structures and their\nhigh inference costs, rendering them impractical for handling large-scale\ngraphs. Inspired by human approaches to graph problems, we introduce a novel\nframework, PIE (Pseudocode-Injection-Enhanced LLM Reasoning for Graph\nComputational Tasks), which consists of three key steps: problem understanding,\nprompt design, and code generation. In this framework, LLMs are tasked with\nunderstanding the problem and extracting relevant information to generate\ncorrect code. The responsibility for analyzing the graph structure and\nexecuting the code is delegated to the interpreter. We inject task-related\npseudocodes into the prompts to further assist the LLMs in generating efficient\ncode. We also employ cost-effective trial-and-error techniques to ensure that\nthe LLM-generated code executes correctly. Unlike other methods that require\ninvoking LLMs for each individual test case, PIE only calls the LLM during the\ncode generation phase, allowing the generated code to be reused and\nsignificantly reducing inference costs. Extensive experiments demonstrate that\nPIE outperforms existing baselines in terms of both accuracy and computational\nefficiency.", "link": "http://arxiv.org/abs/2501.13731v1", "categories": ["cs.CL", "cs.AI"]}, {"title": "Scalable Safe Multi-Agent Reinforcement Learning for Multi-Agent System", "authors": ["Haikuo Du", "Fandi Gou", "Yunze Cai"], "published": "2025-01-23T15:01:19Z", "summary": "Safety and scalability are two critical challenges faced by practical\nMulti-Agent Systems (MAS). However, existing Multi-Agent Reinforcement Learning\n(MARL) algorithms that rely solely on reward shaping are ineffective in\nensuring safety, and their scalability is rather limited due to the fixed-size\nnetwork output. To address these issues, we propose a novel framework, Scalable\nSafe MARL (SS-MARL), to enhance the safety and scalability of MARL methods.\nLeveraging the inherent graph structure of MAS, we design a multi-layer message\npassing network to aggregate local observations and communications of varying\nsizes. Furthermore, we develop a constrained joint policy optimization method\nin the setting of local observation to improve safety. Simulation experiments\ndemonstrate that SS-MARL achieves a better trade-off between optimality and\nsafety compared to baselines, and its scalability significantly outperforms the\nlatest methods in scenarios with a large number of agents. The feasibility of\nour method is also verified by hardware implementation with Mecanum-wheeled\nvehicles.", "link": "http://arxiv.org/abs/2501.13727v1", "categories": ["cs.MA", "cs.AI"]}, {"title": "RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented\n  Generation", "authors": ["Shi-Qi Yan", "Zhen-Hua Ling"], "published": "2025-01-23T14:58:56Z", "summary": "While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing\nexternal knowledge, its generation process heavily depends on the quality and\naccuracy of the retrieved context. Large language models (LLMs) struggle to\nevaluate the correctness of non-parametric knowledge retrieved externally when\nit differs from internal memorization, leading to knowledge conflicts during\nresponse generation. To this end, we introduce the Retrieval Preference\nOptimization (RPO), a lightweight and effective alignment method to adaptively\nleverage multi-source knowledge based on retrieval relevance. An implicit\nrepresentation of retrieval relevance is derived and incorporated into the\nreward model to integrate retrieval evaluation and response generation into a\nsingle model, solving the problem that previous methods necessitate the\nadditional procedure to assess the retrieval quality. Notably, RPO is the only\nRAG-dedicated alignment approach that quantifies the awareness of retrieval\nrelevance in training, overcoming mathematical obstacles. Experiments on four\ndatasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any\nextra component, exhibiting its robust generalization.", "link": "http://arxiv.org/abs/2501.13726v1", "categories": ["cs.CL"]}, {"title": "You Only Crash Once v2: Perceptually Consistent Strong Features for\n  One-Stage Domain Adaptive Detection of Space Terrain", "authors": ["Timothy Chase Jr", "Christopher Wilson", "Karthik Dantu"], "published": "2025-01-23T14:58:49Z", "summary": "The in-situ detection of planetary, lunar, and small-body surface terrain is\ncrucial for autonomous spacecraft applications, where learning-based computer\nvision methods are increasingly employed to enable intelligence without prior\ninformation or human intervention. However, many of these methods remain\ncomputationally expensive for spacecraft processors and prevent real-time\noperation. Training of such algorithms is additionally complex due to the\nscarcity of labeled data and reliance on supervised learning approaches.\nUnsupervised Domain Adaptation (UDA) offers a promising solution by\nfacilitating model training with disparate data sources such as simulations or\nsynthetic scenes, although UDA is difficult to apply to celestial environments\nwhere challenging feature spaces are paramount. To alleviate such issues, You\nOnly Crash Once (YOCOv1) has studied the integration of Visual Similarity-based\nAlignment (VSA) into lightweight one-stage object detection architectures to\nimprove space terrain UDA. Although proven effective, the approach faces\nnotable limitations, including performance degradations in multi-class and\nhigh-altitude scenarios. Building upon the foundation of YOCOv1, we propose\nnovel additions to the VSA scheme that enhance terrain detection capabilities\nunder UDA, and our approach is evaluated across both simulated and real-world\ndata. Our second YOCO rendition, YOCOv2, is capable of achieving\nstate-of-the-art UDA performance on surface terrain detection, where we\nshowcase improvements upwards of 31% compared with YOCOv1 and terrestrial\nstate-of-the-art. We demonstrate the practical utility of YOCOv2 with\nspacecraft flight hardware performance benchmarking and qualitative evaluation\nof NASA mission data.", "link": "http://arxiv.org/abs/2501.13725v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"]}, {"title": "Musical ethnocentrism in Large Language Models", "authors": ["Anna Kruspe"], "published": "2025-01-23T14:50:37Z", "summary": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments.", "link": "http://arxiv.org/abs/2501.13720v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"]}, {"title": "A Mutual Information Perspective on Multiple Latent Variable Generative\n  Models for Positive View Generation", "authors": ["Dario Serez", "Marco Cristani", "Alessio Del Bue", "Vittorio Murino", "Pietro Morerio"], "published": "2025-01-23T14:46:38Z", "summary": "In image generation, Multiple Latent Variable Generative Models (MLVGMs)\nemploy multiple latent variables to gradually shape the final images, from\nglobal characteristics to finer and local details (e.g., StyleGAN, NVAE),\nemerging as powerful tools for diverse applications. Yet their generative\ndynamics and latent variable utilization remain only empirically observed. In\nthis work, we propose a novel framework to systematically quantify the impact\nof each latent variable in MLVGMs, using Mutual Information (MI) as a guiding\nmetric. Our analysis reveals underutilized variables and can guide the use of\nMLVGMs in downstream applications.\n  With this foundation, we introduce a method for generating synthetic data for\nSelf-Supervised Contrastive Representation Learning (SSCRL). By leveraging the\nhierarchical and disentangled variables of MLVGMs, and guided by the previous\nanalysis, we apply tailored latent perturbations to produce diverse views for\nSSCRL, without relying on real data altogether.\n  Additionally, we introduce a Continuous Sampling (CS) strategy, where the\ngenerator dynamically creates new samples during SSCRL training, greatly\nincreasing data variability. Our comprehensive experiments demonstrate the\neffectiveness of these contributions, showing that MLVGMs' generated views\ncompete on par with or even surpass views generated from real data.\n  This work establishes a principled approach to understanding and exploiting\nMLVGMs, advancing both generative modeling and self-supervised learning.", "link": "http://arxiv.org/abs/2501.13718v1", "categories": ["cs.CV"]}, {"title": "Skin Disease Detection and Classification of Actinic Keratosis and\n  Psoriasis Utilizing Deep Transfer Learning", "authors": ["Fahud Ahmmed", "Md. Zaheer Raihan", "Kamnur Nahar", "D. M. Asadujjaman", "Md. Mahfujur Rahman", "Abdullah Tamim"], "published": "2025-01-23T14:43:53Z", "summary": "Skin diseases can arise from infections, allergies, genetic factors,\nautoimmune disorders, hormonal imbalances, or environmental triggers such as\nsun damage and pollution. Some skin diseases, such as Actinic Keratosis and\nPsoriasis, can be fatal if not treated in time. Early identification is\ncrucial, but the diagnostic methods for these conditions are often expensive\nand not widely accessible. In this study, we propose a novel and efficient\nmethod for diagnosing skin diseases using deep learning techniques. This\napproach employs a modified VGG16 Convolutional Neural Network (CNN) model. The\nmodel includes several convolutional layers and utilizes ImageNet weights with\nmodified top layers. The top layer is updated with fully connected layers and a\nfinal softmax activation layer to classify skin diseases. The dataset used,\ntitled \"Skin Disease Dataset,\" is publicly available. While the VGG16\narchitecture does not include data augmentation by default, preprocessing\ntechniques such as rotation, shifting, and zooming were applied to augment the\ndata prior to model training. The proposed methodology achieved 90.67% accuracy\nusing the modified VGG16 model, demonstrating its reliability in classifying\nskin diseases. The promising results highlight the potential of this approach\nfor real-world applications.", "link": "http://arxiv.org/abs/2501.13713v1", "categories": ["cs.CV", "cs.AI", "68T07", "J.3"]}, {"title": "Formally Verified Neurosymbolic Trajectory Learning via Tensor-based\n  Linear Temporal Logic on Finite Traces", "authors": ["Mark Chevallier", "Filip Smola", "Richard Schmoetten", "Jacques D. Fleuriot"], "published": "2025-01-23T14:43:12Z", "summary": "We present a novel formalisation of tensor semantics for linear temporal\nlogic on finite traces (LTLf), with formal proofs of correctness carried out in\nthe theorem prover Isabelle/HOL. We demonstrate that this formalisation can be\nintegrated into a neurosymbolic learning process by defining and verifying a\ndifferentiable loss function for the LTLf constraints, and automatically\ngenerating an implementation that integrates with PyTorch. We show that, by\nusing this loss, the process learns to satisfy pre-specified logical\nconstraints. Our approach offers a fully rigorous framework for constrained\ntraining, eliminating many of the inherent risks of ad-hoc, manual\nimplementations of logical aspects directly in an \"unsafe\" programming language\nsuch as Python, while retaining efficiency in implementation.", "link": "http://arxiv.org/abs/2501.13712v1", "categories": ["cs.AI", "cs.LG", "cs.LO"]}, {"title": "YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised\n  Re-ID", "authors": ["I\u00f1aki Erregue", "Kamal Nasrollahi", "Sergio Escalera"], "published": "2025-01-23T14:38:40Z", "summary": "We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT)\nsolution that combines real-time object detection with self-supervised\nRe-Identification (Re-ID). By incorporating a dedicated Re-ID branch into\nYOLO11s, our model performs Joint Detection and Embedding (JDE), generating\nappearance features for each detection. The Re-ID branch is trained in a fully\nself-supervised setting while simultaneously training for detection,\neliminating the need for costly identity-labeled datasets. The triplet loss,\nwith hard positive and semi-hard negative mining strategies, is used for\nlearning discriminative embeddings. Data association is enhanced with a custom\ntracking implementation that successfully integrates motion, appearance, and\nlocation cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20\nbenchmarks, surpassing existing JDE methods in terms of FPS and using up to ten\ntimes fewer parameters. Thus, making our method a highly attractive solution\nfor real-world applications.", "link": "http://arxiv.org/abs/2501.13710v1", "categories": ["cs.CV", "cs.AI"]}, {"title": "Regularizing cross entropy loss via minimum entropy and K-L divergence", "authors": ["Abdulrahman Oladipupo Ibraheem"], "published": "2025-01-23T14:38:05Z", "summary": "I introduce two novel loss functions for classification in deep learning. The\ntwo loss functions extend standard cross entropy loss by regularizing it with\nminimum entropy and Kullback-Leibler (K-L) divergence terms. The first of the\ntwo novel loss functions is termed mixed entropy loss (MIX-ENT for short),\nwhile the second one is termed minimum entropy regularized cross-entropy loss\n(MIN-ENT for short). The MIX-ENT function introduces a regularizer that can be\nshown to be equivalent to the sum of a minimum entropy term and a K-L\ndivergence term. However, it should be noted that the K-L divergence term here\nis different from that in the standard cross-entropy loss function, in the\nsense that it swaps the roles of the target probability and the hypothesis\nprobability. The MIN-ENT function simply adds a minimum entropy regularizer to\nthe standard cross entropy loss function. In both MIX-ENT and MIN-ENT, the\nminimum entropy regularizer minimizes the entropy of the hypothesis probability\ndistribution which is output by the neural network. Experiments on the\nEMNIST-Letters dataset shows that my implementation of MIX-ENT and MIN-ENT lets\nthe VGG model climb from its previous 3rd position on the paperswithcode\nleaderboard to reach the 2nd position on the leaderboard, outperforming the\nSpinal-VGG model in so doing. Specifically, using standard cross-entropy, VGG\nachieves 95.86% while Spinal-VGG achieves 95.88% classification accuracies,\nwhereas using VGG (without Spinal-VGG) our MIN-ENT achieved 95.933%, while our\nMIX-ENT achieved 95.927% accuracies. The pre-trained models for both MIX-ENT\nand MIN-ENT are at https://github.com/rahmanoladi/minimum entropy project.", "link": "http://arxiv.org/abs/2501.13709v1", "categories": ["cs.CV", "cs.LG"]}, {"title": "EventVL: Understand Event Streams via Multimodal Large Language Model", "authors": ["Pengteng Li", "Yunfan Lu", "Pinghao Song", "Wuyang Li", "Huizai Yao", "Hui Xiong"], "published": "2025-01-23T14:37:21Z", "summary": "The event-based Vision-Language Model (VLM) recently has made good progress\nfor practical vision tasks. However, most of these works just utilize CLIP for\nfocusing on traditional perception tasks, which obstruct model understanding\nexplicitly the sufficient semantics and context from event streams. To address\nthe deficiency, we propose EventVL, the first generative event-based MLLM\n(Multimodal Large Language Model) framework for explicit semantic\nunderstanding. Specifically, to bridge the data gap for connecting different\nmodalities semantics, we first annotate a large event-image/video-text dataset,\ncontaining almost 1.4 million high-quality pairs of data, which enables\neffective learning across various scenes, e.g., drive scene or human motion.\nAfter that, we design Event Spatiotemporal Representation to fully explore the\ncomprehensive information by diversely aggregating and segmenting the event\nstream. To further promote a compact semantic space, Dynamic Semantic Alignment\nis introduced to improve and complete sparse semantic spaces of events.\nExtensive experiments show that our EventVL can significantly surpass existing\nMLLM baselines in event captioning and scene description generation tasks. We\nhope our research could contribute to the development of the event vision\ncommunity.", "link": "http://arxiv.org/abs/2501.13707v1", "categories": ["cs.CV", "cs.AI"]}, {"title": "A real-time battle situation intelligent awareness system based on\n  Meta-learning & RNN", "authors": ["Yuchun Li", "Zihan Lin", "Xize Wang", "Chunyang Liu", "Liaoyuan Wu", "Fang Zhang"], "published": "2025-01-23T14:35:19Z", "summary": "In modern warfare, real-time and accurate battle situation analysis is\ncrucial for making strategic and tactical decisions. The proposed real-time\nbattle situation intelligent awareness system (BSIAS) aims at meta-learning\nanalysis and stepwise RNN (recurrent neural network) modeling, where the former\ncarries out the basic processing and analysis of battlefield data, which\nincludes multi-steps such as data cleansing, data fusion, data mining and\ncontinuously updates, and the latter optimizes the battlefield modeling by\nstepwise capturing the temporal dependencies of data set. BSIAS can predict the\npossible movement from any side of the fence and attack routes by taking a\nsimulated battle as an example, which can be an intelligent support platform\nfor commanders to make scientific decisions during wartime. This work delivers\nthe potential application of integrated BSIAS in the field of battlefield\ncommand & analysis engineering.", "link": "http://arxiv.org/abs/2501.13704v1", "categories": ["cs.LG", "cs.NA", "math.NA"]}, {"title": "GenTL: A General Transfer Learning Model for Building Thermal Dynamics", "authors": ["Fabian Raisch", "Thomas Krug", "Christoph Goebel", "Benjamin Tischler"], "published": "2025-01-23T14:34:55Z", "summary": "Transfer Learning (TL) is an emerging field in modeling building thermal\ndynamics. This method reduces the data required for a data-driven model of a\ntarget building by leveraging knowledge from a source building. Consequently,\nit enables the creation of data-efficient models that can be used for advanced\ncontrol and fault detection & diagnosis. A major limitation of the TL approach\nis its inconsistent performance across different sources. Although accurate\nsource-building selection for a target is crucial, it remains a persistent\nchallenge.\n  We present GenTL, a general transfer learning model for single-family houses\nin Central Europe. GenTL can be efficiently fine-tuned to a large variety of\ntarget buildings. It is pretrained on a Long Short-Term Memory (LSTM) network\nwith data from 450 different buildings. The general transfer learning model\neliminates the need for source-building selection by serving as a universal\nsource for fine-tuning. Comparative analysis with conventional single-source to\nsingle-target TL demonstrates the efficacy and reliability of the general\npretraining approach. Testing GenTL on 144 target buildings for fine-tuning\nreveals an average prediction error (RMSE) reduction of 42.1 % compared to\nfine-tuning single-source models.", "link": "http://arxiv.org/abs/2501.13703v1", "categories": ["eess.SY", "cs.LG", "cs.SY"]}, {"title": "DI-BENCH: Benchmarking Large Language Models on Dependency Inference\n  with Testable Repositories at Scale", "authors": ["Linghao Zhang", "Junhao Wang", "Shilin He", "Chaoyun Zhang", "Yu Kang", "Bowen Li", "Jiaheng Wen", "Chengxing Xie", "Maoquan Wang", "Yufan Huang", "Elsie Nallipogu", "Qingwei Lin", "Yingnong Dang", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "published": "2025-01-23T14:27:11Z", "summary": "Large Language Models have advanced automated software development, however,\nit remains a challenge to correctly infer dependencies, namely, identifying the\ninternal components and external packages required for a repository to\nsuccessfully run. Existing studies highlight that dependency-related issues\ncause over 40\\% of observed runtime errors on the generated repository. To\naddress this, we introduce DI-BENCH, a large-scale benchmark and evaluation\nframework specifically designed to assess LLMs' capability on dependency\ninference. The benchmark features 581 repositories with testing environments\nacross Python, C#, Rust, and JavaScript. Extensive experiments with textual and\nexecution-based metrics reveal that the current best-performing model achieves\nonly a 42.9% execution pass rate, indicating significant room for improvement.\nDI-BENCH establishes a new viewpoint for evaluating LLM performance on\nrepositories, paving the way for more robust end-to-end software synthesis.", "link": "http://arxiv.org/abs/2501.13699v1", "categories": ["cs.CL", "cs.SE"]}, {"title": "The First Indoor Pathloss Radio Map Prediction Challenge", "authors": ["Stefanos Bakirtzis", "\u00c7a\u011fkan Yapar", "Kehai Qiu", "Ian Wassell", "Jie Zhang"], "published": "2025-01-23T14:25:14Z", "summary": "To encourage further research and to facilitate fair comparisons in the\ndevelopment of deep learning-based radio propagation models, in the less\nexplored case of directional radio signal emissions in indoor propagation\nenvironments, we have launched the ICASSP 2025 First Indoor Pathloss Radio Map\nPrediction Challenge. This overview paper describes the indoor path loss\nprediction problem, the datasets used, the Challenge tasks, and the evaluation\nmethodology. Finally, the results of the Challenge and a summary of the\nsubmitted methods are presented.", "link": "http://arxiv.org/abs/2501.13698v1", "categories": ["eess.SP", "cs.LG"]}, {"title": "Training-Free Consistency Pipeline for Fashion Repose", "authors": ["Potito Aghilar", "Vito Walter Anelli", "Michelantonio Trizio", "Tommaso Di Noia"], "published": "2025-01-23T14:17:01Z", "summary": "Recent advancements in diffusion models have significantly broadened the\npossibilities for editing images of real-world objects. However, performing\nnon-rigid transformations, such as changing the pose of objects or image-based\nconditioning, remains challenging. Maintaining object identity during these\nedits is difficult, and current methods often fall short of the precision\nneeded for industrial applications, where consistency is critical.\nAdditionally, fine-tuning diffusion models requires custom training data, which\nis not always accessible in real-world scenarios. This work introduces\nFashionRepose, a training-free pipeline for non-rigid pose editing specifically\ndesigned for the fashion industry. The approach integrates off-the-shelf models\nto adjust poses of long-sleeve garments, maintaining identity and branding\nattributes. FashionRepose uses a zero-shot approach to perform these edits in\nnear real-time, eliminating the need for specialized training. consistent image\nediting. The solution holds potential for applications in the fashion industry\nand other fields demanding identity preservation in image editing.", "link": "http://arxiv.org/abs/2501.13692v1", "categories": ["cs.CV", "cs.AI", "cs.SE"]}, {"title": "Variational U-Net with Local Alignment for Joint Tumor Extraction and\n  Registration (VALOR-Net) of Breast MRI Data Acquired at Two Different Field\n  Strengths", "authors": ["Muhammad Shahkar Khan", "Haider Ali", "Laura Villazan Garcia", "Noor Badshah", "Siegfried Trattnig", "Florian Schwarzhans", "Ramona Woitek", "Olgica Zaric"], "published": "2025-01-23T14:15:54Z", "summary": "Background: Multiparametric breast MRI data might improve tumor diagnostics,\ncharacterization, and treatment planning. Accurate alignment and delineation of\nimages acquired at different field strengths such as 3T and 7T, remain\nchallenging research tasks. Purpose: To address alignment challenges and enable\nconsistent tumor segmentation across different MRI field strengths. Study type:\nRetrospective. Subjects: Nine female subjects with breast tumors were involved:\nsix histologically proven invasive ductal carcinomas (IDC) and three\nfibroadenomas. Field strength/sequence: Imaging was performed at 3T and 7T\nscanners using post-contrast T1-weighted three-dimensional time-resolved\nangiography with stochastic trajectories (TWIST) sequence. Assessments: The\nmethod's performance for joint image registration and tumor segmentation was\nevaluated using several quantitative metrics, including signal-to-noise ratio\n(PSNR), structural similarity index (SSIM), normalized cross-correlation (NCC),\nDice coefficient, F1 score, and relative sum of squared differences (rel SSD).\nStatistical tests: The Pearson correlation coefficient was used to test the\nrelationship between the registration and segmentation metrics. Results: When\ncalculated for each subject individually, the PSNR was in a range from 27.5 to\n34.5 dB, and the SSIM was from 82.6 to 92.8%. The model achieved an NCC from\n96.4 to 99.3% and a Dice coefficient of 62.9 to 95.3%. The F1 score was between\n55.4 and 93.2% and the rel SSD was in the range of 2.0 and 7.5%. The\nsegmentation metrics Dice and F1 Score are highly correlated (0.995), while a\nmoderate correlation between NCC and SSIM (0.681) was found for registration.\nData conclusion: Initial results demonstrate that the proposed method may be\nfeasible in providing joint tumor segmentation and registration of MRI data\nacquired at different field strengths.", "link": "http://arxiv.org/abs/2501.13690v1", "categories": ["eess.IV", "cs.CV"]}, {"title": "Question Answering on Patient Medical Records with Private Fine-Tuned\n  LLMs", "authors": ["Sara Kothari", "Ayush Gupta"], "published": "2025-01-23T14:13:56Z", "summary": "Healthcare systems continuously generate vast amounts of electronic health\nrecords (EHRs), commonly stored in the Fast Healthcare Interoperability\nResources (FHIR) standard. Despite the wealth of information in these records,\ntheir complexity and volume make it difficult for users to retrieve and\ninterpret crucial health insights. Recent advances in Large Language Models\n(LLMs) offer a solution, enabling semantic question answering (QA) over medical\ndata, allowing users to interact with their health records more effectively.\nHowever, ensuring privacy and compliance requires edge and private deployments\nof LLMs.\n  This paper proposes a novel approach to semantic QA over EHRs by first\nidentifying the most relevant FHIR resources for a user query (Task1) and\nsubsequently answering the query based on these resources (Task2). We explore\nthe performance of privately hosted, fine-tuned LLMs, evaluating them against\nbenchmark models such as GPT-4 and GPT-4o. Our results demonstrate that\nfine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by\n0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we\nexamine advanced aspects of LLM usage, including sequential fine-tuning, model\nself-evaluation (narcissistic evaluation), and the impact of training data size\non performance. The models and datasets are available here:\nhttps://huggingface.co/genloop", "link": "http://arxiv.org/abs/2501.13687v1", "categories": ["cs.CL", "cs.AI"]}, {"title": "Unlearning Clients, Features and Samples in Vertical Federated Learning", "authors": ["Ayush K. Varshney", "Konstantinos Vandikas", "Vicen\u00e7 Torra"], "published": "2025-01-23T14:10:02Z", "summary": "Federated Learning (FL) has emerged as a prominent distributed learning\nparadigm. Within the scope of privacy preservation, information privacy\nregulations such as GDPR entitle users to request the removal (or unlearning)\nof their contribution from a service that is hosting the model. For this\npurpose, a server hosting an ML model must be able to unlearn certain\ninformation in cases such as copyright infringement or security issues that can\nmake the model vulnerable or impact the performance of a service based on that\nmodel. While most unlearning approaches in FL focus on Horizontal FL (HFL),\nwhere clients share the feature space and the global model, Vertical FL (VFL)\nhas received less attention from the research community. VFL involves clients\n(passive parties) sharing the sample space among them while not having access\nto the labels. In this paper, we explore unlearning in VFL from three\nperspectives: unlearning clients, unlearning features, and unlearning samples.\nTo unlearn clients and features we introduce VFU-KD which is based on knowledge\ndistillation (KD) while to unlearn samples, VFU-GA is introduced which is based\non gradient ascent. To provide evidence of approximate unlearning, we utilize\nMembership Inference Attack (MIA) to audit the effectiveness of our unlearning\napproach. Our experiments across six tabular datasets and two image datasets\ndemonstrate that VFU-KD and VFU-GA achieve performance comparable to or better\nthan both retraining from scratch and the benchmark R2S method in many cases,\nwith improvements of $(0-2\\%)$. In the remaining cases, utility scores remain\ncomparable, with a modest utility loss ranging from $1-5\\%$. Unlike existing\nmethods, VFU-KD and VFU-GA require no communication between active and passive\nparties during unlearning. However, they do require the active party to store\nthe previously communicated embeddings.", "link": "http://arxiv.org/abs/2501.13683v1", "categories": ["cs.LG", "cs.AI"]}, {"title": "Collective Memory and Narrative Cohesion: A Computational Study of\n  Palestinian Refugee Oral Histories in Lebanon", "authors": ["Ghadeer Awwad", "Lavinia Dunagan", "David Gamba", "Tamara N. Rayan"], "published": "2025-01-23T14:07:49Z", "summary": "This study uses the Palestinian Oral History Archive (POHA) to investigate\nhow Palestinian refugee groups in Lebanon sustain a cohesive collective memory\nof the Nakba through shared narratives. Grounded in Halbwachs' theory of group\nmemory, we employ statistical analysis of pairwise similarity of narratives,\nfocusing on the influence of shared gender and location. We use textual\nrepresentation and semantic embeddings of narratives to represent the\ninterviews themselves. Our analysis demonstrates that shared origin is a\npowerful determinant of narrative similarity across thematic keywords,\nlandmarks, and significant figures, as well as in semantic embeddings of the\nnarratives. Meanwhile, shared residence fosters cohesion, with its impact\nsignificantly amplified when paired with shared origin. Additionally, women's\nnarratives exhibit heightened thematic cohesion, particularly in recounting\nexperiences of the British occupation, underscoring the gendered dimensions of\nmemory formation. This research deepens the understanding of collective memory\nin diasporic settings, emphasizing the critical role of oral histories in\nsafeguarding Palestinian identity and resisting erasure.", "link": "http://arxiv.org/abs/2501.13682v1", "categories": ["cs.CL", "J.4; I.2.7"]}, {"title": "HumorReject: Decoupling LLM Safety from Refusal Prefix via A Little\n  Humor", "authors": ["Zihui Wu", "Haichang Gao", "Jiacheng Luo", "Zhaoxiang Liu"], "published": "2025-01-23T14:02:51Z", "summary": "Large Language Models (LLMs) commonly rely on explicit refusal prefixes for\nsafety, making them vulnerable to prefix injection attacks. We introduce\nHumorReject, a novel data-driven approach that fundamentally reimagines LLM\nsafety by decoupling it from refusal prefixes through the use of humor as an\nindirect refusal strategy. Rather than explicitly rejecting harmful\ninstructions, HumorReject responds with contextually appropriate humor that\nnaturally defuses potentially dangerous requests while maintaining engaging\ninteractions. Our approach effectively addresses the common \"over-defense\"\nissues in existing safety mechanisms, demonstrating superior robustness against\nvarious attack vectors while preserving natural and high-quality interactions\non legitimate tasks. Our findings suggest that innovations at the data level\nare even more fundamental than the alignment algorithm itself in achieving\neffective LLM safety, opening new directions for developing more resilient and\nuser-friendly AI systems.", "link": "http://arxiv.org/abs/2501.13677v1", "categories": ["cs.LG", "cs.CR"]}, {"title": "Certified Robustness Under Bounded Levenshtein Distance", "authors": ["Elias Abad Rocamora", "Grigorios G. Chrysos", "Volkan Cevher"], "published": "2025-01-23T13:58:53Z", "summary": "Text classifiers suffer from small perturbations, that if chosen\nadversarially, can dramatically change the output of the model. Verification\nmethods can provide robustness certificates against such adversarial\nperturbations, by computing a sound lower bound on the robust accuracy.\nNevertheless, existing verification methods incur in prohibitive costs and\ncannot practically handle Levenshtein distance constraints. We propose the\nfirst method for computing the Lipschitz constant of convolutional classifiers\nwith respect to the Levenshtein distance. We use these Lipschitz constant\nestimates for training 1-Lipschitz classifiers. This enables computing the\ncertified radius of a classifier in a single forward pass. Our method, LipsLev,\nis able to obtain $38.80$% and $13.93$% verified accuracy at distance $1$ and\n$2$ respectively in the AG-News dataset, while being $4$ orders of magnitude\nfaster than existing approaches. We believe our work can open the door to more\nefficient verification in the text domain.", "link": "http://arxiv.org/abs/2501.13676v1", "categories": ["cs.LG", "cs.AI", "cs.CL"]}, {"title": "How to Complete Domain Tuning while Keeping General Ability in LLM:\n  Adaptive Layer-wise and Element-wise Regularization", "authors": ["Shezheng Song", "Hao Xu", "Jun Ma", "Shasha Li", "Long Peng", "Qian Wan", "Xiaodong Liu", "Jie Yu"], "published": "2025-01-23T13:54:53Z", "summary": "Large Language Models (LLMs) exhibit strong general-purpose language\ncapabilities. However, fine-tuning these models on domain-specific tasks often\nleads to catastrophic forgetting, where the model overwrites or loses essential\nknowledge acquired during pretraining. This phenomenon significantly limits the\nbroader applicability of LLMs. To address this challenge, we propose a novel\napproach to compute the element-wise importance of model parameters crucial for\npreserving general knowledge during fine-tuning. Our method utilizes a\ndual-objective optimization strategy: (1) regularization loss to retain the\nparameter crucial for general knowledge; (2) cross-entropy loss to adapt to\ndomain-specific tasks. Additionally, we introduce layer-wise coefficients to\naccount for the varying contributions of different layers, dynamically\nbalancing the dual-objective optimization. Extensive experiments on scientific,\nmedical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our\napproach mitigates catastrophic forgetting while enhancing model adaptability.\nCompared to previous methods, our solution is approximately 20 times faster and\nrequires only 10%-15% of the storage, highlighting the practical efficiency.\nThe code will be released.", "link": "http://arxiv.org/abs/2501.13669v1", "categories": ["cs.CL", "cs.AI"]}, {"title": "MPG-SAM 2: Adapting SAM 2 with Mask Priors and Global Context for\n  Referring Video Object Segmentation", "authors": ["Fu Rong", "Meng Lan", "Qian Zhang", "Lefei Zhang"], "published": "2025-01-23T13:53:33Z", "summary": "Referring video object segmentation (RVOS) aims to segment objects in a video\naccording to textual descriptions, which requires the integration of multimodal\ninformation and temporal dynamics perception. The Segment Anything Model 2 (SAM\n2) has shown great effectiveness across various video segmentation tasks.\nHowever, its application to offline RVOS is challenged by the translation of\nthe text into effective prompts and a lack of global context awareness. In this\npaper, we propose a novel RVOS framework, termed MPG-SAM 2, to address these\nchallenges. Specifically, MPG-SAM 2 employs a unified multimodal encoder to\njointly encode video and textual features, generating semantically aligned\nvideo and text embeddings, along with multimodal class tokens. A mask prior\ngenerator utilizes the video embeddings and class tokens to create pseudo masks\nof target objects and global context. These masks are fed into the prompt\nencoder as dense prompts along with multimodal class tokens as sparse prompts\nto generate accurate prompts for SAM 2. To provide the online SAM 2 with a\nglobal view, we introduce a hierarchical global-historical aggregator, which\nallows SAM 2 to aggregate global and historical information of target objects\nat both pixel and object levels, enhancing the target representation and\ntemporal consistency. Extensive experiments on several RVOS benchmarks\ndemonstrate the superiority of MPG-SAM 2 and the effectiveness of our proposed\nmodules.", "link": "http://arxiv.org/abs/2501.13667v1", "categories": ["cs.CV"]}, {"title": "LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning\n  Approach for Multi-modal Large Language Models", "authors": ["Yizheng Sun", "Yanze Xin", "Hao Li", "Jingyuan Sun", "Chenghua Lin", "Riza Batista-Navarro"], "published": "2025-01-23T13:31:51Z", "summary": "Multi-modal Large Language Models (MLLMs) have achieved remarkable success by\nintegrating visual and textual modalities. However, they incur significant\ncomputational overhead due to the large number of vision tokens processed,\nlimiting their practicality in resource-constrained environments. We introduce\nLanguage-Guided Vision Token Pruning (LVPruning) for MLLMs, an effective yet\nsimple method that significantly reduces the computational burden while\npreserving model performance. LVPruning employs cross-attention modules to\ncompute the importance of vision tokens based on their interaction with\nlanguage tokens, determining which to prune. Importantly, LVPruning can be\nintegrated without modifying the original MLLM parameters, which makes\nLVPruning simple to apply or remove. Our experiments show that LVPruning can\neffectively reduce up to 90% of vision tokens by the middle layer of LLaVA-1.5,\nresulting in a 62.1% decrease in inference Tera Floating-Point Operations Per\nSecond (TFLOPs), with an average performance loss of just 0.45% across nine\nmulti-modal benchmarks.", "link": "http://arxiv.org/abs/2501.13652v1", "categories": ["cs.CL"]}, {"title": "Revisiting Online Learning Approach to Inverse Linear Optimization: A\n  Fenchel--Young Loss Perspective and Gap-Dependent Regret Analysis", "authors": ["Shinsaku Sakaue", "Han Bao", "Taira Tsuchiya"], "published": "2025-01-23T13:27:14Z", "summary": "This paper revisits the online learning approach to inverse linear\noptimization studied by B\\\"armann et al. (2017), where the goal is to infer an\nunknown linear objective function of an agent from sequential observations of\nthe agent's input-output pairs. First, we provide a simple understanding of the\nonline learning approach through its connection to online convex optimization\nof \\emph{Fenchel--Young losses}. As a byproduct, we present an offline\nguarantee on the \\emph{suboptimality loss}, which measures how well predicted\nobjectives explain the agent's choices, without assuming the optimality of the\nagent's choices. Second, assuming that there is a gap between optimal and\nsuboptimal objective values in the agent's decision problems, we obtain an\nupper bound independent of the time horizon $T$ on the sum of suboptimality and\n\\emph{estimate losses}, where the latter measures the quality of solutions\nrecommended by predicted objectives. Interestingly, our gap-dependent analysis\nachieves a faster rate than the standard $O(\\sqrt{T})$ regret bound by\nexploiting structures specific to inverse linear optimization, even though\nneither the loss functions nor their domains enjoy desirable properties, such\nas strong convexity.", "link": "http://arxiv.org/abs/2501.13648v1", "categories": ["cs.LG"]}, {"title": "Enhancing Medical Image Analysis through Geometric and Photometric\n  transformations", "authors": ["Khadija Rais", "Mohamed Amroune", "Mohamed Yassine Haouam"], "published": "2025-01-23T13:21:14Z", "summary": "Medical image analysis suffers from a lack of labeled data due to several\nchallenges including patient privacy and lack of experts. Although some AI\nmodels only perform well with large amounts of data, we will move to data\naugmentation where there is a solution to improve the performance of our models\nand increase the dataset size through traditional or advanced techniques. In\nthis paper, we evaluate the effectiveness of data augmentation techniques on\ntwo different medical image datasets. In the first step, we applied some\ntransformation techniques to the skin cancer dataset containing benign and\nmalignant classes. Then, we trained the convolutional neural network (CNN) on\nthe dataset before and after augmentation, which significantly improved test\naccuracy from 90.74% to 96.88% and decreased test loss from 0.7921 to 0.1468\nafter augmentation. In the second step, we used the Mixup technique by mixing\ntwo random images and their corresponding masks using the retina and blood\nvessels dataset, then we trained the U-net model and obtained the Dice\ncoefficient which increased from 0 before augmentation to 0.4163 after\naugmentation. The result shows the effect of using data augmentation to\nincrease the dataset size on the classification and segmentation performance.", "link": "http://arxiv.org/abs/2501.13643v1", "categories": ["eess.IV", "cs.CV"]}]}}